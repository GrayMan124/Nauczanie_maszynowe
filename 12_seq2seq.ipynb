{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETFpFlCmeuSB"
      },
      "source": [
        "# Modele Seq2Seq i atencja\n",
        "Poniższy notebook jest inspirowany tym tutorialem PyTorcha: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html. Zachęcam żeby zajrzeć tam po więcej informacji.\n",
        "\n",
        "W tym notebooku będziemy próbować rozwiązać problem automatycznego tłumaczenia zdań z jednego języka naturalnego na drugi -- konkretniej z języka polskiego na angielski. Dla przykładu model otrzymujący zdanie:\n",
        "\n",
        "> Myślę, że mnie okłamałeś\n",
        "\n",
        "Powinien zwrócić zdanie\n",
        "> I think you lied to me.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Do7k4lf0zsxu"
      },
      "source": [
        "# Importy i przygotowanie danych\n",
        "Poniżej znajdują się importy bibliotek potrzebnych do rozwiązania problemu a także skrypt do ładowania zbioru danych zawierającego pary zdań w języku polskim i angielskim. Poniższy kod można odpalić i schować, ale zachęcamy do zaznajomienia się z tym jak wygląda obróbka danych.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zCAgsgj3d7Kf"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Z1yFSn0ld7Ki",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "e44f1dee-c559-42e3-b3b6-69eb11114306"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-2-d60674a3ec40>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbleu_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchtext'"
          ]
        }
      ],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import unicodedata\n",
        "import string\n",
        "import numpy as np\n",
        "import re\n",
        "import random\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUVJHRTXKoWh"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/gmum/ml2021-22/master/lab/resources/eng-pol.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KnEaLlNpire"
      },
      "source": [
        "Poniżej przygotowujemy klasę `Lang` która będzie służyła jako struktura do obsługiwania naszego języka (osobna dla angielskiego i polskiego w naszym przypadku). Do każdego słowa w języku przypisujemy indeks (liczbę porządkową identyfikującą słowo). Dodatkowo definiujemy trzy dodatkowe indeksy:\n",
        "\n",
        "* 0 dla początku zdania (Start of Sentence, SOS)\n",
        "* 1 dla końca zdania (End of Sentence, EOS)\n",
        "* 2 dla paddingu (\"pustych\" wartości). Wartościami tymi będziemy wypełniać zdania w batchu tak, żeby wszystkie były równej długości -- dzięki temu łatwiej będzie zrównoleglić przetwarzanie ich na GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3DtzxXTd7Kk"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "PAD_token = 2\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2: \"PAD\"}\n",
        "        self.n_words = 3 # Count SOS, EOS and PAD\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrV27D5Xryh1"
      },
      "source": [
        "Funkcje do normalizowania wchodzących zdań - zamieniamy Unicode na ASCII, zamieniamy wszystkie wielkie litery na małe itd."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLLuOE80d7Kk"
      },
      "outputs": [],
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = s.replace(\"ł\", \"l\")\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s\n",
        "\n",
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open('eng-pol.txt', encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "    lines = lines[1:]  # Skip first line with attributions.\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')[1::2]] for l in lines]\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ld3aJ9Nd7Kl"
      },
      "source": [
        "Wyrzućmy zdania które są zbyt długie (ponad 20 słów)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6fbLbiQd7Km"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 20\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) <= MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) <= MAX_LENGTH\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csmhl8Iod7Km"
      },
      "source": [
        "Pełny proces przetwarzania danych wygląda następująco:\n",
        "\n",
        "- Wczytujemy plik z danymi, dzielimy go na pary zdań.\n",
        "- Normalizujemy tekst\n",
        "- Zamieniamy zdania w listy słów.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRd-fNMAd7Km"
      },
      "outputs": [],
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(pairs[0])\n",
        "    print(f\"Read {len(pairs)} sentence pairs\")\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(f\"Trimmed to {len(pairs)} sentence pairs\")\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('pol', 'eng', True)\n",
        "print(\"Przykładowe pary zdań:\")\n",
        "for _ in range(3):\n",
        "    print(random.choice(pairs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQc1t2ArshOa"
      },
      "source": [
        "Na koniec definiujemy jeszcze funkcje, które pozwolą nam zamienić zdania w tensory, które nasza sieć będzie w stanie zrozumieć."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leinIgjGd7Kq"
      },
      "outputs": [],
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1)\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)\n",
        "\n",
        "\n",
        "def pad_sequences(data_batch):\n",
        "    pl_batch, en_batch = [], []\n",
        "    for pl_sentence, en_sentence in data_batch:\n",
        "        pl_batch += [pl_sentence]\n",
        "        en_batch += [en_sentence]\n",
        "    pl_batch = pad_sequence(pl_batch, padding_value=PAD_token, batch_first=True)\n",
        "    en_batch = pad_sequence(en_batch, padding_value=PAD_token, batch_first=True)\n",
        "    return pl_batch, en_batch\n",
        "\n",
        "def prepare_dataset(batch_size):\n",
        "    rng = np.random.RandomState(567)\n",
        "    indices = np.arange(len(pairs))\n",
        "    rng.shuffle(indices)\n",
        "    train_indices = indices[:int(len(pairs) * 0.8)]\n",
        "    test_indices = indices[int(len(pairs) * 0.8):]\n",
        "    train_pairs = list(pairs[idx] for idx in train_indices)\n",
        "    test_pairs = list(pairs[idx] for idx in test_indices)\n",
        "    tensor_train_pairs = [tensorsFromPair(pairs[idx]) for idx in train_indices]\n",
        "    tensor_test_pairs = [tensorsFromPair(pairs[idx]) for idx in test_indices]\n",
        "    reference_translation = test_pairs\n",
        "\n",
        "    # Output in natural language?\n",
        "\n",
        "    train_loader = DataLoader(tensor_train_pairs, batch_size=batch_size,\n",
        "                            shuffle=True, collate_fn=pad_sequences)\n",
        "    test_loader = DataLoader(tensor_test_pairs, batch_size=batch_size,\n",
        "                            shuffle=True, collate_fn=pad_sequences)\n",
        "    return train_pairs, test_pairs, train_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf844auxd7Km"
      },
      "source": [
        "# Model Seq2Seq\n",
        "\n",
        "\n",
        "\n",
        "W tym celu wykorzystany rekurencyjne sieci neuronowe (RNN-y), które poznaliśmy na poprzednich zajęciach. Konkretniej zbudujemy za ich pomocą model Sequence to Sequence (Seq2Seq), w której wykorzystamy dwie sieci rekurencyjne:\n",
        "1. Enkoder, który będzie przyjmował kolejno słowa ze zdania wejściowego i kompresował informacje o nich w swoim stanie ukrytym.\n",
        "2. Dekoder, który będzie generował kolejne słowa w języku docelowym. \n",
        "\n",
        "![seq2seq](https://docs.chainer.org/en/stable/_images/seq2seq.png)\n",
        "Źródło: https://docs.chainer.org/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxaO-7_LtPUk"
      },
      "source": [
        "## Funkcje pomocnicze i ewaluacyjne"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04Zm6_MRd7Kq"
      },
      "outputs": [],
      "source": [
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "\n",
        "def predict(encoder, decoder, inputs, targets=None, max_len=MAX_LENGTH):\n",
        "    batch_size = inputs.size(0)\n",
        "\n",
        "    encoder_outputs, encoder_hidden = encoder(inputs)\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]] * batch_size, device=device)\n",
        "    decoder_hidden = encoder_hidden\n",
        "    decoder_output, decoder_attention = decoder(\n",
        "        decoder_input,\n",
        "        decoder_hidden,\n",
        "        targets=targets,\n",
        "        max_len=max_len,\n",
        "        encoder_outputs=encoder_outputs)\n",
        "    return decoder_output, decoder_attention\n",
        "\n",
        "def translate(encoder, decoder, sentence, show_attention=True):\n",
        "    inputs = tensorFromSentence(input_lang, sentence).unsqueeze(0).cuda()\n",
        "    decoder_output, decoder_attention = predict(encoder, decoder, inputs)\n",
        "\n",
        "    decoded_words = []\n",
        "    for word in decoder_output[0]:\n",
        "        top_word = word.argmax(-1).item()\n",
        "        decoded_words.append(output_lang.index2word[top_word])\n",
        "        if top_word == EOS_token:\n",
        "            break\n",
        "\n",
        "    if decoder_attention is not None and show_attention:\n",
        "        # [out_words, in_words]\n",
        "        att = decoder_attention.cpu().detach().numpy()\n",
        "        att = att[0, :len(decoded_words), :]\n",
        "        fig, ax = plt.subplots()\n",
        "\n",
        "        im = ax.imshow(att, vmin=0, vmax=1)\n",
        "        ax.xaxis.tick_top()\n",
        "        ax.set_xticklabels([''] + sentence.split(' ') +\n",
        "                        ['EOS'], rotation=90)\n",
        "        ax.set_yticklabels([''] + decoded_words)\n",
        "        divider = make_axes_locatable(ax)\n",
        "        cax = divider.append_axes('right', size='5%', pad=0.05)\n",
        "\n",
        "        fig.colorbar(im, cax=cax, orientation='vertical')\n",
        "        \n",
        "        ax.tick_params(axis='both', which='major', labelsize=12)\n",
        "\n",
        "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    return decoded_words\n",
        "\n",
        "def batch_translate(encoder, decoder, batch):\n",
        "    decoder_output, decoder_attention = predict(encoder, decoder, batch)\n",
        "\n",
        "    predicted_sentences = []\n",
        "\n",
        "    # TODO: potentially paralellize?\n",
        "    for batch_idx in range(len(batch)):\n",
        "        predicted_words = []\n",
        "        for word in decoder_output[batch_idx]:\n",
        "            top_word = word.argmax(-1).item()\n",
        "            if top_word == EOS_token:\n",
        "                break\n",
        "            predicted_words.append(output_lang.index2word[top_word])\n",
        "\n",
        "        predicted_sentences.append(predicted_words)\n",
        "\n",
        "    return predicted_sentences\n",
        "\n",
        "def dataset_translate(encoder, decoder, loader):\n",
        "    predicted_sentences = []\n",
        "    reference_sentences = [] \n",
        "    for batch_in, batch_out in loader:\n",
        "        translated = batch_translate(encoder, decoder, batch_in)\n",
        "        predicted_sentences.extend(translated)\n",
        "\n",
        "        # TODO: move to a separate file?\n",
        "        reference_words = []\n",
        "        for sentence_idx, sentence in enumerate(batch_out):\n",
        "            decoded_sentence = []\n",
        "            for word in sentence:\n",
        "                if word.item() == EOS_token:\n",
        "                    break\n",
        "                decoded_sentence.append(output_lang.index2word[word.item()])\n",
        "            reference_sentences.append(decoded_sentence)\n",
        "    \n",
        "    return predicted_sentences, reference_sentences\n",
        "\n",
        "\n",
        "def translate_randomly(encoder, decoder, pairs, n=10):\n",
        "    # TODO: reuse translate_given_pairs\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words = translate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')\n",
        "\n",
        "def translate_given_pairs(encoder, decoder, pairs):\n",
        "    for pair in pairs:\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words = translate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')\n",
        "        \n",
        "def plot_results(bleus, losses):\n",
        "    sns.set_style('whitegrid')\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(20, 6))\n",
        "\n",
        "    axes[0].plot(np.arange(len(bleus)), bleus)\n",
        "    axes[0].set_xlabel(\"Epoka\")\n",
        "    axes[0].set_ylabel(\"BLEU\")\n",
        "    axes[1].plot(np.arange(len(losses)), losses)\n",
        "    axes[1].set_xlabel(\"Epoka\")\n",
        "    axes[1].set_ylabel(\"Koszt na zbiorze treningowym\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKEO71h8_mvq"
      },
      "source": [
        "## Pętla trenująca"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2hJ24pEd7Kq"
      },
      "outputs": [],
      "source": [
        "def train(encoder, decoder, lr=0.01, batch_size=256, teacher_forcing_ratio=0.5, epochs_num=100, clipping=1.0):\n",
        "\n",
        "    # Prepare dataset, loss functions, optimizer\n",
        "    train_pairs, test_pairs, train_loader, test_loader = prepare_dataset(batch_size)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_token)\n",
        "\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=lr)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr)\n",
        "\n",
        "    encoder.cuda()\n",
        "    decoder.cuda()\n",
        "\n",
        "    bleus = []\n",
        "    train_losses = []\n",
        "\n",
        "    for epoch in range(epochs_num + 1):\n",
        "\n",
        "        # Training\n",
        "        epoch_train_loss = 0.\n",
        "        for in_batch, out_batch in train_loader:\n",
        "            in_batch, out_batch = in_batch.cuda(), out_batch.cuda()\n",
        "\n",
        "            encoder_optimizer.zero_grad()\n",
        "            decoder_optimizer.zero_grad()\n",
        "        \n",
        "            teacher_inputs = out_batch if random.random() < teacher_forcing_ratio else None\n",
        "        \n",
        "            decoder_output, decoded_attention = predict(\n",
        "                encoder, decoder, in_batch,\n",
        "                targets=teacher_inputs,\n",
        "                max_len=out_batch.size(1)\n",
        "            )\n",
        "\n",
        "            loss = criterion(decoder_output.transpose(1, 2), out_batch)\n",
        "            \n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            nn.utils.clip_grad_norm_(encoder.parameters(), clipping)\n",
        "            nn.utils.clip_grad_norm_(decoder.parameters(), clipping)\n",
        "\n",
        "            encoder_optimizer.step()\n",
        "            decoder_optimizer.step()\n",
        "\n",
        "            epoch_train_loss += loss.item()\n",
        "\n",
        "        # Evaluation\n",
        "        if epoch % 1 == 0:\n",
        "            with torch.no_grad():\n",
        "                print(\"=\" * 25, \"Translation test\", \"=\" * 25)\n",
        "                translate_randomly(encoder, decoder, test_pairs, n=5)\n",
        "\n",
        "            pred_sentences, ref_sentences = dataset_translate(encoder, decoder, test_loader)\n",
        "            bleu_val = bleu_score(pred_sentences, [[sentence] for sentence in ref_sentences])\n",
        "            print(\"=\" * 25, f\"BLEU: {bleu_val}\", \"=\" * 25)\n",
        "            bleus += [bleu_val]\n",
        "\n",
        "        mean_train_loss = epoch_train_loss / len(train_loader)\n",
        "        train_losses += [mean_train_loss]\n",
        "        print(f\"Epoch: {epoch}. Train loss: {mean_train_loss}\")\n",
        "    return bleus, train_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkvoS5hCjgI2"
      },
      "source": [
        "# Zadanie 1 - Dekoder w Seq2Seq (4 pkt.)\n",
        "\n",
        "W tym zadaniu należy zaimplementować dekoder z modelu Seq2Seq. Kod enkodera jest dostępny poniżej i ma Państwu ułatwić odpowiednie zaimplementowanie dekodera.\n",
        "\n",
        "\n",
        "\n",
        "Dekoder otrzymuje na wejściu następujące argumenty:\n",
        "- `input` - tensor o wymiarach `[batch_size, 1]` zawierający słowo `<BOS>`. Powinno być podane w pierwszym kroku wykonywania dekodera.\n",
        "- `hidden` - ostatnia reprezentacja ukryta z enkodera .\n",
        "- `targets` - `None` albo `torch.tensor` o wymiarach `[batch_size, seq_len]` zawierający indeksy słów w języku docelowym. Jeżeli jest podany to należy zaimplementować teacher forcing na jego podstawie.\n",
        "- `max_len` - Długość sekwencji, którą mamy zwrócić.\n",
        "- `encoder_outputs` - w tym zadaniu ten argument należy zignorować, przyda się dopiero w kolejnym zadaniu.\n",
        "\n",
        "Dekoder ma zwrócić dwie zmienne:\n",
        "- `output` - tensor o wymiarach `[batch_size, max_len, vocab_size]` reprezentujące logity, które po zaaplikowaniu softmaksa (co będzie zrobione już poza dekoderem) będą reprezentowały prawdopodobieństwa słów przewidzianych przez nasz dekoder.\n",
        "- `attention_weights` - w tym zadaniu należy zawsze zwracać `None`.\n",
        "\n",
        "Architektura głowy klasyfikacyjnej jest dowolna, natomiast zalecamy sieć z jedną warstwą ukrytą: `[hidden_size, hidden_size, vocab_size]` i aktywacją tanh.\n",
        "\n",
        "\n",
        "**HINT 1**: Warto pamiętać o argumencie `batch_first=True` przy definiowaniu RNN-a.\n",
        "\n",
        "**HINT 2**: W enkoderze mogliśmy użyć jednego wywołania klasy GRU, jako że od razu mieliśmy wszystkie wejścia (słowa języka wejściowego). W przypadku dekodera nie jest to możliwe, jako że wejściem w kroku `t+1` jest wyjście z kroku `t`. Oznacza to że prawdopodobnie potrzebna będzie pętla `for`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "I6FSVbB0d7Kn"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.rnn_cell = nn.GRU(embedding_size, hidden_size, batch_first=True)\n",
        "\n",
        "    def forward(self, input):\n",
        "        embedded = self.embedding(input)\n",
        "        output, hidden = self.rnn_cell(embedded)\n",
        "        return output, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "MiBbM1IVd7Ko"
      },
      "outputs": [],
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        \n",
        "        self.embedding_size=embedding_size\n",
        "        self.hidden_size=hidden_size\n",
        "        self.vocab_size=vocab_size\n",
        "\n",
        "        self.embedding=nn.Embedding(vocab_size,embedding_size)\n",
        "        self.rnn_cell=nn.GRU(embedding_size,hidden_size,batch_first=True)\n",
        "        self.linear=nn.Sequential(\n",
        "            nn.Linear(hidden_size,hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_size,hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_size,vocab_size)\n",
        "        )\n",
        "        self.softmax=nn.LogSoftmax(dim=-1)\n",
        "\n",
        "    def forward(self, input, hidden, targets=None, max_len=None, encoder_outputs=None):\n",
        "        output=torch.zeros((max_len,input.shape[0],self.vocab_size)).cuda()\n",
        "        batch_size=input.shape[0]\n",
        "        for i in range(max_len):\n",
        "          embedded=self.embedding(input)\n",
        "          rnn,hidden = self.rnn_cell(embedded, hidden)\n",
        "          out = self.linear(rnn)\n",
        "          out=out.squeeze(1)\n",
        "          out=out.float()\n",
        "          output[i,:,:]=out\n",
        "          if(targets==None):\n",
        "            out=self.softmax(out)\n",
        "            input=out.argmax(dim=-1)\n",
        "            input=torch.reshape(input,(batch_size,1))\n",
        "          else:\n",
        "            input=targets[:,i]\n",
        "            input=torch.reshape(input,(batch_size,1))\n",
        "        output=output.view(batch_size,max_len,self.vocab_size)\n",
        "\n",
        "        return output, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KiePQdpivCd",
        "outputId": "bc56c83a-bfd0-47eb-85be-64434dd372ff",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================= Translation test =========================\n",
            "> tom moglby sie nie zgodzic .\n",
            "= tom might not agree .\n",
            "< EOS\n",
            "\n",
            "> chcialbym sie z toba zobaczyc zanim pojade do europy .\n",
            "= i would like to see you before leaving for europe .\n",
            "< EOS\n",
            "\n",
            "> pij mniej i spij wiecej .\n",
            "= drink less and sleep more .\n",
            "< EOS\n",
            "\n",
            "> stare dywany sa cenniejsze niz nowe dywany .\n",
            "= older carpets are more valuable than newer carpets .\n",
            "< EOS\n",
            "\n",
            "> czytales kiedys ksiazke po francusku ?\n",
            "= have you ever read a book in french ?\n",
            "< EOS\n",
            "\n",
            "========================= BLEU: 0.0 =========================\n",
            "Epoch: 0. Train loss: 5.892219353747624\n",
            "========================= Translation test =========================\n",
            "> ziemia nasza matka .\n",
            "= mother earth .\n",
            "< EOS\n",
            "\n",
            "> film mi sie podobal .\n",
            "= i liked the movie .\n",
            "< EOS\n",
            "\n",
            "> nie zrobie tego .\n",
            "= i won t do it .\n",
            "< EOS\n",
            "\n",
            "> juz sie lepiej czujesz ?\n",
            "= you re feeling better now aren t you ?\n",
            "< EOS\n",
            "\n",
            "> czym dzieckiem jestes ?\n",
            "= whose child are you ?\n",
            "< EOS\n",
            "\n",
            "========================= BLEU: 0.0 =========================\n",
            "Epoch: 1. Train loss: 5.741962084206202\n",
            "========================= Translation test =========================\n",
            "> uczniowie z utesknieniem wyczekuja letnich wakacji .\n",
            "= the students are looking forward to the summer vacation .\n",
            "< EOS\n",
            "\n",
            "> nie potrafi ani czytac ani pisac .\n",
            "= he can neither read nor write .\n",
            "< EOS\n",
            "\n",
            "> nigdy jeszcze nie spotkalem nikogo kto by mnie tak uszczesliwial jak ty mnie .\n",
            "= i ve never met anyone who makes me as happy as you make me .\n",
            "< EOS\n",
            "\n",
            "> tom nie poslubil mary .\n",
            "= tom didn t marry mary .\n",
            "< EOS\n",
            "\n",
            "> teraz ja czytam ty czytasz i on czyta wszyscy czytamy .\n",
            "= now i m reading you re reading and he s reading we re all reading .\n",
            "< EOS\n",
            "\n",
            "========================= BLEU: 0.0 =========================\n",
            "Epoch: 2. Train loss: 5.7441929642872145\n",
            "========================= Translation test =========================\n",
            "> tom jest od switu na nogach .\n",
            "= tom has been up since dawn .\n",
            "< EOS\n",
            "\n",
            "> co to za zamieszanie ?\n",
            "= what s the commotion ?\n",
            "< EOS\n",
            "\n",
            "> londyn jest jednym z najwiekszych miast na swiecie .\n",
            "= london is one of the largest cities in the world .\n",
            "< EOS\n",
            "\n",
            "> jakie bylo pierwsze francuskie slowo ktorego sie nauczylas ?\n",
            "= what was the first word you learned in french ?\n",
            "< EOS\n",
            "\n",
            "> to juz koniec .\n",
            "= this is the end .\n",
            "< EOS\n",
            "\n",
            "========================= BLEU: 0.0 =========================\n",
            "Epoch: 3. Train loss: 5.724718560454666\n",
            "========================= Translation test =========================\n",
            "> tom boi sie ze moze byc zwolniony z pracy .\n",
            "= tom is afraid that he might get laid off .\n",
            "< EOS\n",
            "\n",
            "> dlaczego nie przymierzysz tego zoltego swetra ?\n",
            "= why don t you try on this yellow sweater ?\n",
            "< EOS\n",
            "\n",
            "> tom jest tancerzem .\n",
            "= tom is a dancer .\n",
            "< EOS\n",
            "\n",
            "> od kiedy tak sie dzieje ?\n",
            "= when did it first start to happen ?\n",
            "< EOS\n",
            "\n",
            "> jestes nam winien trzysta dolarow .\n",
            "= you owe us three hundred dollars .\n",
            "< EOS\n",
            "\n",
            "========================= BLEU: 0.0 =========================\n",
            "Epoch: 4. Train loss: 5.72730815538796\n"
          ]
        }
      ],
      "source": [
        "hidden_size = 1024\n",
        "embedding_size = 512\n",
        "lr = 1e-3\n",
        "\n",
        "encoder = EncoderRNN(input_lang.n_words, embedding_size, hidden_size).to(device)\n",
        "decoder = DecoderRNN(output_lang.n_words, embedding_size, hidden_size).to(device)\n",
        "\n",
        "bleus, losses = train(encoder, decoder, batch_size=512, lr=lr, epochs_num=10, clipping=0.1)\n",
        "plot_results(bleus, losses)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Niestety pomimo prób model nie chce się uczyc i wypisuje tylko EOS, próbowałem zmieniać lr ale nic to nie dało, próbowałem też użyć funkcji relu na embeddingu,\n",
        "# lub zmieniać architektrę klasyfikatora, nadal bez rezultatów. Wydaje mi się że gdzieś albo przy force teachingu, albo przy przekazywaniu inputu z decodera do wejścia następnego obrotu\n",
        "# jest jakiś błąd, ale niestety nie jestem w stanie go znaleźć"
      ],
      "metadata": {
        "id": "e05ylF75DJM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crrwsvmgGHix"
      },
      "source": [
        "# Zadanie 2 - Atencja w RNN-ach (3 pkt.)\n",
        "\n",
        "![seq2seq z dekoderem](https://www.researchgate.net/profile/Chandan_Reddy6/publication/329464533/figure/fig3/AS:701043021197314@1544153089772/An-attention-based-seq2seq-model.ppm)\n",
        "\n",
        "Źródło: https://github.com/google/seq2seq\n",
        "\n",
        "W tym zadaniu należy napisać kod nowego dekodera, który ma działać podobnie jak dekoder w poprzednim zadaniu, ale jednocześnie ma wykorzystywać mechanizm atencji.\n",
        "\n",
        "W normalnym dekoderze, w kroku `t` wejściem do komórki GRU (pomijamy tutaj przekazywanie stanu ukrytego) była wyłącznie zembeddowana reprezentacja $\\bar{y}_t$. W dekoderze z atencją na wejściu podawna będzie konkatenacja tego wektora oraz specjalnego wektora $z_t$ stworzonego na podstawie wyjść z enkodera: $\\tilde{h}_t = [\\bar{y}_t, z_t]$. \n",
        "\n",
        "Wektor $z_t$ jest pozyskiwany za pomocą mechanizmu atencji. Intuicyjnie chcielibyśmy w nim zebrać informacje z enkodera, które będą najistotniejsze przy dekodowaniu aktualnego słowa. Przyjmijmy, że mamy funkcję alignmentu $a(h, e)$, która jest nam w stanie powiedzieć jak bardzo podobne do siebie są stan ukryty dekodera $h$ oraz reprezentacja słowa $e$.\n",
        "\n",
        "Wtedy \n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "w_i &= \\frac{ \\exp(a(h, e_i)) }{\\sum_{j} \\exp(a(h, e_j))} \\\\\n",
        "z_t &= \\sum_i e_i \\cdot w_i\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "\n",
        "W naszym wypadku funkcja alignmentu $a(h, e)$ ma być siecią neuronową z dwoma warstwami o kolejnych wymiarach: `[embedding_size + hidden_size, hidden_size, 1]` i aktywacją tanh po pierwszej warstwie.\n",
        "\n",
        "Argumenty wejściowe i wyjściowe z dekodera są takie same jak w poprzednim z zadaniu z wyjątkiem:\n",
        "- Tym razem na wejściu otrzymujemy tensor `encoder_outputs` o wymiarach `[batch_size, encoder_seq_len, hidden_size]`. To są reprezentacje $e_i$, które należy wykorzystać w mechanizmie atencji.\n",
        "- Tym razem na wyjściu `attention_weights` powinno być tensorem o wymiarach `[batch_size, decoder_seq_len, encoder_seq_len]` zawierającym wagi $w_i$. **HINT:** wartości tego tensora powinny się sumować do jedynki na ostatnim wymiarze."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5bETuU9Hd7Kp"
      },
      "outputs": [],
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size=hidden_size\n",
        "        self.vocab_size=vocab_size\n",
        "\n",
        "        self.embedding=nn.Embedding(vocab_size,embedding_size)\n",
        "        self.attn=nn.Sequential(\n",
        "            nn.Linear(embedding_size + hidden_size,hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_size,hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_size,1)\n",
        "        )\n",
        "        self.rnn_cell=nn.GRU(embedding_size + embedding_size,hidden_size,batch_first=True)\n",
        "\n",
        "        self.linear=nn.Sequential(\n",
        "            nn.Linear(hidden_size,hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_size,hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_size,vocab_size)\n",
        "        )\n",
        "    def forward(self, input, hidden, targets=None, max_len=None, encoder_outputs=None):\n",
        "        output=torch.zeros((max_len,input.shape[0],self.vocab_size)).cuda()\n",
        "        batch_size=input.shape[0]\n",
        "        for i in range(max_len):\n",
        "          embedded=self.embedding(input)\n",
        "          rnn,hidden = self.rnn_cell(embedded, hidden)\n",
        "          attn=attn(encoder_outputs)\n",
        "\n",
        "          out = self.linear(rnn)\n",
        "          out=out.squeeze(1)\n",
        "          out=out.float()\n",
        "          output[i,:,:]=out\n",
        "          if(targets==None):\n",
        "            out=self.softmax(out)\n",
        "            input=out.argmax(dim=-1)\n",
        "            input=torch.reshape(input,(batch_size,1))\n",
        "          else:\n",
        "            input=targets[:,i]\n",
        "            input=torch.reshape(input,(batch_size,1))\n",
        "        output=output.view(batch_size,max_len,self.vocab_size)\n",
        "\n",
        "\n",
        "        seq_att_weights=0\n",
        "        return output, seq_att_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "mipeLXX9fx_7",
        "outputId": "b687a68b-a78b-4b55-9486-7310b8b4dec4",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-5-a198dea2a363>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e-3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mclip\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEncoderRNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_lang\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAttnDecoderRNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_lang\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'input_lang' is not defined"
          ]
        }
      ],
      "source": [
        "hidden_size = 1024\n",
        "embedding_size = 512\n",
        "lr = 1e-3\n",
        "clip = 0.1\n",
        "encoder = EncoderRNN(input_lang.n_words, embedding_size, hidden_size).to(device)\n",
        "decoder = AttnDecoderRNN(output_lang.n_words, embedding_size, hidden_size).to(device)\n",
        "\n",
        "bleus, losses = train(encoder, decoder, lr=lr, batch_size=512, epochs_num=10, clipping=0.1)\n",
        "plot_results(bleus, losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPmEeponfgob"
      },
      "source": [
        "# Powiązana literatura\n",
        "\n",
        "* [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)\n",
        "* [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)\n",
        "\n",
        "## Przydatne tutoriale\n",
        "\n",
        "* https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
        "* https://github.com/bentrevett/pytorch-seq2seq\n",
        "* https://github.com/gmum/AppliedDL2020/tree/master/Week%207 - materiały z kursu Applied Deep Learning prowadzonego w semestrze letnim\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAoJGLVhV_-y"
      },
      "source": [
        "# Zadanie dodatkowe: Transformer (7 pkt.)\n",
        "\n",
        "Na podstawie pracy [Attention is All You Need](https://arxiv.org/abs/1706.03762) oraz strony [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention) zaimplementuj transformera działającego na powyższym zadaniu tłumaczenia z polskiego na angielski.\n",
        "\n",
        "![transformer](http://nlp.seas.harvard.edu/images/the-annotated-transformer_14_0.png)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "12_seq2seq.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}