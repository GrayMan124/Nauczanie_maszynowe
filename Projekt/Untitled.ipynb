{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "192cc0a6-da42-4dc0-8d8d-684720951cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import Normalize\n",
    "from collections import namedtuple\n",
    "\n",
    "torch.cuda.is_available()\n",
    "\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3f16154a-d3ca-4839-8391-4e476780075e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters=pd.read_csv(\"data/reut_data.csv\",sep='|')\n",
    "\n",
    "sx5e_data=pd.read_csv(\"data\\SX5E.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c6615779-c0bc-42c2-9473-b5695b3f766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters.drop(columns='column1',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fac27d2d-ad4f-4a4f-a812-284a1ab292ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters.drop_duplicates(subset =\"Headline\",\n",
    "                     keep = 'first', inplace = True)\n",
    "reuters.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ec427e34-b95b-4764-8d96-f491abddc20c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', ' Open', ' High', ' Low', ' Close'], dtype='object')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sx5e_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0085198e-ed03-491b-8a77-2da513083a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "sx5e_data['Date']=pd.to_datetime(sx5e_data.Date)\n",
    "sx5e_data = sx5e_data.sort_index(axis=0 ,ascending=False)\n",
    "sx5e_data.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3df21932-62a4-421d-a8a8-43c89e9fa467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>2964.96</td>\n",
       "      <td>2964.96</td>\n",
       "      <td>2964.96</td>\n",
       "      <td>2964.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>2976.09</td>\n",
       "      <td>3017.80</td>\n",
       "      <td>2974.86</td>\n",
       "      <td>3017.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>3014.56</td>\n",
       "      <td>3025.60</td>\n",
       "      <td>3006.47</td>\n",
       "      <td>3012.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-06</td>\n",
       "      <td>3013.06</td>\n",
       "      <td>3016.83</td>\n",
       "      <td>2997.05</td>\n",
       "      <td>3009.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-07</td>\n",
       "      <td>2997.37</td>\n",
       "      <td>3013.90</td>\n",
       "      <td>2979.80</td>\n",
       "      <td>3007.34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date     Open     High      Low    Close\n",
       "0 2010-01-01  2964.96  2964.96  2964.96  2964.96\n",
       "1 2010-01-04  2976.09  3017.80  2974.86  3017.80\n",
       "2 2010-01-05  3014.56  3025.60  3006.47  3012.36\n",
       "3 2010-01-06  3013.06  3016.83  2997.05  3009.66\n",
       "4 2010-01-07  2997.37  3013.90  2979.80  3007.34"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sx5e_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0cbe3071-cc29-4d19-b5c4-5b8260dc236b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-12-10</td>\n",
       "      <td>REFILE-UPDATE 2-European stocks slip on Omicro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-12-10</td>\n",
       "      <td>Tech stocks drag European shares ahead of U.S....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-12-09</td>\n",
       "      <td>UPDATE 2-European shares fall again on Omicron...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-12-09</td>\n",
       "      <td>European shares rebound as vaccine reassurance...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-12-08</td>\n",
       "      <td>UPDATE 2-European shares end lower as tech, lu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date                                           Headline\n",
       "0  2021-12-10  REFILE-UPDATE 2-European stocks slip on Omicro...\n",
       "1  2021-12-10  Tech stocks drag European shares ahead of U.S....\n",
       "2  2021-12-09  UPDATE 2-European shares fall again on Omicron...\n",
       "3  2021-12-09  European shares rebound as vaccine reassurance...\n",
       "4  2021-12-08  UPDATE 2-European shares end lower as tech, lu..."
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "41e5c0c4-bfa3-4d42-8565-c36061a9578f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date                                           Headline\n",
      "0  2021-12-10  REFILE-UPDATE 2-European stocks slip on Omicro...\n",
      "1  2021-12-10  Tech stocks drag European shares ahead of U.S....\n",
      "2  2021-12-09  UPDATE 2-European shares fall again on Omicron...\n",
      "3  2021-12-09  European shares rebound as vaccine reassurance...\n",
      "4  2021-12-08  UPDATE 2-European shares end lower as tech, lu...\n",
      "        Date     Open     High      Low    Close\n",
      "0 2010-01-01  2964.96  2964.96  2964.96  2964.96\n",
      "1 2010-01-04  2976.09  3017.80  2974.86  3017.80\n",
      "2 2010-01-05  3014.56  3025.60  3006.47  3012.36\n",
      "3 2010-01-06  3013.06  3016.83  2997.05  3009.66\n",
      "4 2010-01-07  2997.37  3013.90  2979.80  3007.34\n",
      "(3106, 5)\n",
      "12659\n",
      "Tech stocks drag European shares ahead of U.S. inflation data\n"
     ]
    }
   ],
   "source": [
    "print(reuters.head())\n",
    "print(sx5e_data.head())\n",
    "print(sx5e_data.shape)\n",
    "print(len(reuters))\n",
    "print(reuters['Headline'][1])\n",
    "reuters.dropna(inplace=True)\n",
    "reuters.reset_index(drop=False,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bf2039f4-6b7d-4187-9e38-76ba67d0ade3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-61-4fffbfdca2bc>:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reuters['Headline'][i]=text\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(reuters)):\n",
    "    text=reuters['Headline'][i]\n",
    "    text=text.replace(\"REFILE-UPDATE 2-\",\"\")\n",
    "    text=text.replace(\"UPDATE 2-\",\"\")\n",
    "    text=text.replace(\"UPDATE 1-\",\"\")\n",
    "    text=text.replace(\"UPDATE \",\"\")\n",
    "    text=text.replace(\",\",\"\")\n",
    "    text=text.replace(\".\",\"\")\n",
    "    text=text.replace(\";\",\"\")\n",
    "    text=text.replace(\":\",\"\")\n",
    "    text=text.replace(\"-\",\"\")\n",
    "    text=text.replace(\"European\",\"\")\n",
    "    reuters['Headline'][i]=text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "37d00a0c-b43a-40a2-b44c-71f3b513e22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=reuters['Headline'][1464]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "948e633c-fdb1-46c9-b82d-a3955a411146",
   "metadata": {},
   "outputs": [],
   "source": [
    "sx5e_data.drop(columns=[' Open',' High',' Low'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7efd8afd-474d-447a-a72e-517ba77b39d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>2964.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>3017.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>3012.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-06</td>\n",
       "      <td>3009.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-07</td>\n",
       "      <td>3007.34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date    Close\n",
       "0 2010-01-01  2964.96\n",
       "1 2010-01-04  3017.80\n",
       "2 2010-01-05  3012.36\n",
       "3 2010-01-06  3009.66\n",
       "4 2010-01-07  3007.34"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sx5e_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b7af5ac0-e8ae-4c06-b456-6db63c690389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_fun(data,delta_t):\n",
    "    data1=data.to_numpy()\n",
    "    ret_fun=[]\n",
    "    for i in range(len(data1)-delta_t):\n",
    "        ret_fun.append(100*(data1[i+delta_t,1]-data1[i,1])/data1[i,1])\n",
    "    for i in range(delta_t):\n",
    "        ret_fun.append(None)\n",
    "    ret_fun=ret_fun\n",
    "    return ret_fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "a7c7758a-ffde-4abf-9115-cd14fe9fb9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.08963072142772685\n"
     ]
    }
   ],
   "source": [
    "ret_fun=return_fun(sx5e_data,1)\n",
    "print(ret_fun[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e87d395c-1667-4a69-bd02-719a1304e825",
   "metadata": {},
   "outputs": [],
   "source": [
    "sx5e_data['Return']=ret_fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a0301f9c-ffcb-4b22-a1d0-483201390bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>Return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>2964.96</td>\n",
       "      <td>1.782149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>3017.80</td>\n",
       "      <td>-0.180264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>3012.36</td>\n",
       "      <td>-0.089631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-06</td>\n",
       "      <td>3009.66</td>\n",
       "      <td>-0.077085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-07</td>\n",
       "      <td>3007.34</td>\n",
       "      <td>0.349478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date    Close    Return\n",
       "0 2010-01-01  2964.96  1.782149\n",
       "1 2010-01-04  3017.80 -0.180264\n",
       "2 2010-01-05  3012.36 -0.089631\n",
       "3 2010-01-06  3009.66 -0.077085\n",
       "4 2010-01-07  3007.34  0.349478"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sx5e_data.dropna(inplace=True)\n",
    "sx5e_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "07a27229-a98f-4422-935f-2c8095808194",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {}\n",
    "        self.n_words = 0 \n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "7b9d7c1b-7e22-4cdd-91de-99a07e08835e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ret_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "90482bb6-2844-431a-84ff-67b8f299c1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ret_fun.remove(None)\n",
    "tensor_ret=torch.tensor(ret_fun,dtype=torch.float32)\n",
    "mean_ret=torch.mean(tensor_ret)\n",
    "std_ret=torch.std(tensor_ret)\n",
    "ret_norm=(tensor_ret-mean_ret)/std_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "d27a01e9-1a38-411c-9af8-01e13b31286f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set=[]\n",
    "data=namedtuple('Data',['headline','ret'])\n",
    "for i in range(len(reuters)):\n",
    "    headline=tensorFromSentence(lang,reuters['Headline'][i])\n",
    "    row=sx5e_data.loc[sx5e_data['Date']== reuters['Date'][i]]\n",
    "    ret=row['Return']\n",
    "    if(ret.empty):\n",
    "        continue\n",
    "    ret=torch.tensor(ret.values)\n",
    "    r=(ret-mean_ret)/std_ret\n",
    "    d=(headline,r)\n",
    "    data_set.append(d)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "4cf0533c-afc5-4291-9ff5-016d5e639d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ 0, 14, 28, 29, 23, 30, 31,  1, 32, 33]), tensor([-0.4707], dtype=torch.float64))\n"
     ]
    }
   ],
   "source": [
    "print(data_set[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "551f5f65-7fec-4a16-909b-dfc8c3e6fae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12654\n"
     ]
    }
   ],
   "source": [
    "print(len(data_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "408e0946-62d2-449c-9915-2827795e5c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang=Lang('t')\n",
    "\n",
    "for i in range(len(reuters)):\n",
    "    sentence=reuters['Headline'][i]\n",
    "    lang.addSentence(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "97ebb44f-a507-4b2c-96c7-f0992053ef83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    return torch.tensor(indexes, dtype=torch.long).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b13aa54e-6822-4046-9cf4-39812a8d0d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=reuters['Headline'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d740d7ee-7d96-42a6-b3ff-b481276954ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRECTED stocks slip ahead of PMI US jobs data\n",
      "tensor([308,   1,   2,  15,  16, 399,  17, 309,  19])\n"
     ]
    }
   ],
   "source": [
    "head,ret=data_set[145]\n",
    "print(head)\n",
    "t_text=tensorFromSentence(lang,head)\n",
    "print(t_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "55f43766-8b92-4265-9e8d-d4db2b41a1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4201\n"
     ]
    }
   ],
   "source": [
    "print(lang.n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "741aef14-c890-4c7e-9e16-e49d95cecfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeding=torch.nn.Embedding(lang.n_words,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "d678664e-6804-45bb-acf2-803325a325da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 256])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeding(t_text).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "b6502a33-c311-41fb-b2e3-934e2b825279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 256])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeding(t_text).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "3964b336-d167-43c2-b46b-2e7f32bdf90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextOnlyGRU(nn.Module):\n",
    "    def __init__(self,vocab_size,embedding_size,hidden_size,output_size):\n",
    "        super(TextOnlyGRU,self).__init__()\n",
    "        \n",
    "        self.embedding=nn.Embedding(vocab_size,embedding_size)\n",
    "        self.bi_gru=nn.GRU(embedding_size, hidden_size,2, batch_first=True)\n",
    "        self.fully_connected=nn.Sequential(\n",
    "            nn.Linear(128,64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64,32),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32,1)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        embed=self.embedding(x)\n",
    "        gru_output, h=self.bi_gru(embed)\n",
    "        g=gru_output[:,-1,:]\n",
    "        flat=torch.flatten(gru_output,start_dim=1)\n",
    "        return self.fully_connected(g)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "93ea1135-e248-4be2-a9e8-535197c06d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=TextOnlyGRU(lang.n_words,256,128,1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "d1ca0877-7637-4299-aa56-c3a2a351ccea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=torch.utils.data.DataLoader(data_set, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "bbe304ca-d9ab-42ae-b6cc-1fd8da93275e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0, 14, 20, 21,  3,  4,  5]], device='cuda:0')\n",
      "tensor([[-0.0432]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0, 14, 22, 23, 24, 25, 26, 27]], device='cuda:0')\n",
      "tensor([[-0.0432]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0, 14, 28, 29, 23, 30, 31,  1, 32, 33]], device='cuda:0')\n",
      "tensor([[-0.0432]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[34, 35, 26,  0, 14, 36, 37, 38]], device='cuda:0')\n",
      "tensor([[-0.0432]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[12, 39, 40,  0, 14, 41,  8, 42, 43, 44, 45, 46]], device='cuda:0')\n",
      "tensor([[-0.0432]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0, 14, 47, 44, 48, 49, 23,  4,  5, 50]], device='cuda:0')\n",
      "tensor([[-0.0432]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0, 14, 51,  3, 52,  4, 53, 54, 55]], device='cuda:0')\n",
      "tensor([[-0.0432]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0, 14, 51, 36, 56,  9]], device='cuda:0')\n",
      "tensor([[-0.0432]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0, 14, 28, 57,  9, 58, 59]], device='cuda:0')\n",
      "tensor([[-0.0432]], device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "for i, (x, y) in enumerate(train_loader):\n",
    "    y=y.cuda()\n",
    "    x=x.cuda()\n",
    "    print(x)\n",
    "    print(model(headline_tesnor))\n",
    "    if i % 10 ==8:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "d874590f-d9ff-43ac-a29d-94c2df53fa7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Progress:  0% Loss: 0.030\n",
      "Epoch: 1 Progress:  4% Loss: 0.664\n",
      "Epoch: 1 Progress:  8% Loss: 1.990\n",
      "Epoch: 1 Progress: 12% Loss: 1.487\n",
      "Epoch: 1 Progress: 16% Loss: 1.249\n",
      "Epoch: 1 Progress: 20% Loss: 1.116\n",
      "Epoch: 1 Progress: 24% Loss: 1.292\n",
      "Epoch: 1 Progress: 28% Loss: 1.382\n",
      "Epoch: 1 Progress: 32% Loss: 1.594\n",
      "Epoch: 1 Progress: 36% Loss: 1.534\n",
      "Epoch: 1 Progress: 40% Loss: 1.582\n",
      "Epoch: 1 Progress: 43% Loss: 1.502\n",
      "Epoch: 1 Progress: 47% Loss: 1.438\n",
      "Epoch: 1 Progress: 51% Loss: 1.371\n",
      "Epoch: 1 Progress: 55% Loss: 1.351\n",
      "Epoch: 1 Progress: 59% Loss: 1.342\n",
      "Epoch: 1 Progress: 63% Loss: 1.294\n",
      "Epoch: 1 Progress: 67% Loss: 1.326\n",
      "Epoch: 1 Progress: 71% Loss: 1.351\n",
      "Epoch: 1 Progress: 75% Loss: 1.356\n",
      "Epoch: 1 Progress: 79% Loss: 1.487\n",
      "Epoch: 1 Progress: 83% Loss: 1.491\n",
      "Epoch: 1 Progress: 87% Loss: 1.466\n",
      "Epoch: 1 Progress: 91% Loss: 1.449\n",
      "Epoch: 1 Progress: 95% Loss: 1.448\n",
      "Epoch: 1 Progress: 99% Loss: 1.555\n",
      "\n",
      "Epoch: 1 Loss: tensor(1.5595)\n",
      "\n",
      "Epoch: 2 Progress:  0% Loss: 0.047\n",
      "Epoch: 2 Progress:  4% Loss: 0.607\n",
      "Epoch: 2 Progress:  8% Loss: 1.939\n",
      "Epoch: 2 Progress: 12% Loss: 1.450\n",
      "Epoch: 2 Progress: 16% Loss: 1.215\n",
      "Epoch: 2 Progress: 20% Loss: 1.085\n",
      "Epoch: 2 Progress: 24% Loss: 1.255\n",
      "Epoch: 2 Progress: 28% Loss: 1.350\n",
      "Epoch: 2 Progress: 32% Loss: 1.565\n",
      "Epoch: 2 Progress: 36% Loss: 1.508\n",
      "Epoch: 2 Progress: 40% Loss: 1.556\n",
      "Epoch: 2 Progress: 43% Loss: 1.477\n",
      "Epoch: 2 Progress: 47% Loss: 1.415\n",
      "Epoch: 2 Progress: 51% Loss: 1.347\n",
      "Epoch: 2 Progress: 55% Loss: 1.327\n",
      "Epoch: 2 Progress: 59% Loss: 1.303\n",
      "Epoch: 2 Progress: 63% Loss: 1.255\n",
      "Epoch: 2 Progress: 67% Loss: 1.293\n",
      "Epoch: 2 Progress: 71% Loss: 1.325\n",
      "Epoch: 2 Progress: 75% Loss: 1.322\n",
      "Epoch: 2 Progress: 79% Loss: 1.438\n",
      "Epoch: 2 Progress: 83% Loss: 1.442\n",
      "Epoch: 2 Progress: 87% Loss: 1.417\n",
      "Epoch: 2 Progress: 91% Loss: 1.405\n",
      "Epoch: 2 Progress: 95% Loss: 1.401\n",
      "Epoch: 2 Progress: 99% Loss: 1.486\n",
      "\n",
      "Epoch: 2 Loss: tensor(1.4879)\n",
      "\n",
      "Epoch: 3 Progress:  0% Loss: 0.078\n",
      "Epoch: 3 Progress:  4% Loss: 0.607\n",
      "Epoch: 3 Progress:  8% Loss: 1.815\n",
      "Epoch: 3 Progress: 12% Loss: 1.375\n",
      "Epoch: 3 Progress: 16% Loss: 1.155\n",
      "Epoch: 3 Progress: 20% Loss: 1.020\n",
      "Epoch: 3 Progress: 24% Loss: 1.146\n",
      "Epoch: 3 Progress: 28% Loss: 1.237\n",
      "Epoch: 3 Progress: 32% Loss: 1.421\n",
      "Epoch: 3 Progress: 36% Loss: 1.363\n",
      "Epoch: 3 Progress: 40% Loss: 1.399\n",
      "Epoch: 3 Progress: 43% Loss: 1.333\n",
      "Epoch: 3 Progress: 47% Loss: 1.272\n",
      "Epoch: 3 Progress: 51% Loss: 1.211\n",
      "Epoch: 3 Progress: 55% Loss: 1.195\n",
      "Epoch: 3 Progress: 59% Loss: 1.179\n",
      "Epoch: 3 Progress: 63% Loss: 1.139\n",
      "Epoch: 3 Progress: 67% Loss: 1.183\n",
      "Epoch: 3 Progress: 71% Loss: 1.221\n",
      "Epoch: 3 Progress: 75% Loss: 1.210\n",
      "Epoch: 3 Progress: 79% Loss: 1.328\n",
      "Epoch: 3 Progress: 83% Loss: 1.333\n",
      "Epoch: 3 Progress: 87% Loss: 1.315\n",
      "Epoch: 3 Progress: 91% Loss: 1.301\n",
      "Epoch: 3 Progress: 95% Loss: 1.298\n",
      "Epoch: 3 Progress: 99% Loss: 1.385\n",
      "\n",
      "Epoch: 3 Loss: tensor(1.3894)\n",
      "\n",
      "Epoch: 4 Progress:  0% Loss: 0.032\n",
      "Epoch: 4 Progress:  4% Loss: 0.628\n",
      "Epoch: 4 Progress:  8% Loss: 1.856\n",
      "Epoch: 4 Progress: 12% Loss: 1.403\n",
      "Epoch: 4 Progress: 16% Loss: 1.166\n",
      "Epoch: 4 Progress: 20% Loss: 1.029\n",
      "Epoch: 4 Progress: 24% Loss: 1.131\n",
      "Epoch: 4 Progress: 28% Loss: 1.232\n",
      "Epoch: 4 Progress: 32% Loss: 1.386\n",
      "Epoch: 4 Progress: 36% Loss: 1.323\n",
      "Epoch: 4 Progress: 40% Loss: 1.374\n",
      "Epoch: 4 Progress: 43% Loss: 1.305\n",
      "Epoch: 4 Progress: 47% Loss: 1.248\n",
      "Epoch: 4 Progress: 51% Loss: 1.191\n",
      "Epoch: 4 Progress: 55% Loss: 1.177\n",
      "Epoch: 4 Progress: 59% Loss: 1.163\n",
      "Epoch: 4 Progress: 63% Loss: 1.125\n",
      "Epoch: 4 Progress: 67% Loss: 1.166\n",
      "Epoch: 4 Progress: 71% Loss: 1.202\n",
      "Epoch: 4 Progress: 75% Loss: 1.194\n",
      "Epoch: 4 Progress: 79% Loss: 1.301\n",
      "Epoch: 4 Progress: 83% Loss: 1.310\n",
      "Epoch: 4 Progress: 87% Loss: 1.291\n",
      "Epoch: 4 Progress: 91% Loss: 1.274\n",
      "Epoch: 4 Progress: 95% Loss: 1.273\n",
      "Epoch: 4 Progress: 99% Loss: 1.353\n",
      "\n",
      "Epoch: 4 Loss: tensor(1.3548)\n",
      "\n",
      "Epoch: 5 Progress:  0% Loss: 0.132\n",
      "Epoch: 5 Progress:  4% Loss: 0.617\n",
      "Epoch: 5 Progress:  8% Loss: 1.866\n",
      "Epoch: 5 Progress: 12% Loss: 1.402\n",
      "Epoch: 5 Progress: 16% Loss: 1.164\n",
      "Epoch: 5 Progress: 20% Loss: 1.023\n",
      "Epoch: 5 Progress: 24% Loss: 1.123\n",
      "Epoch: 5 Progress: 28% Loss: 1.217\n",
      "Epoch: 5 Progress: 32% Loss: 1.383\n",
      "Epoch: 5 Progress: 36% Loss: 1.317\n",
      "Epoch: 5 Progress: 40% Loss: 1.361\n",
      "Epoch: 5 Progress: 43% Loss: 1.293\n",
      "Epoch: 5 Progress: 47% Loss: 1.236\n",
      "Epoch: 5 Progress: 51% Loss: 1.176\n",
      "Epoch: 5 Progress: 55% Loss: 1.167\n",
      "Epoch: 5 Progress: 59% Loss: 1.154\n",
      "Epoch: 5 Progress: 63% Loss: 1.116\n",
      "Epoch: 5 Progress: 67% Loss: 1.156\n",
      "Epoch: 5 Progress: 71% Loss: 1.188\n",
      "Epoch: 5 Progress: 75% Loss: 1.186\n",
      "Epoch: 5 Progress: 79% Loss: 1.288\n",
      "Epoch: 5 Progress: 83% Loss: 1.296\n",
      "Epoch: 5 Progress: 87% Loss: 1.278\n",
      "Epoch: 5 Progress: 91% Loss: 1.265\n",
      "Epoch: 5 Progress: 95% Loss: 1.264\n",
      "Epoch: 5 Progress: 99% Loss: 1.352\n",
      "\n",
      "Epoch: 5 Loss: tensor(1.3541)\n",
      "\n",
      "Epoch: 6 Progress:  0% Loss: 0.155\n",
      "Epoch: 6 Progress:  4% Loss: 0.631\n",
      "Epoch: 6 Progress:  8% Loss: 1.840\n",
      "Epoch: 6 Progress: 12% Loss: 1.388\n",
      "Epoch: 6 Progress: 16% Loss: 1.151\n",
      "Epoch: 6 Progress: 20% Loss: 1.019\n",
      "Epoch: 6 Progress: 24% Loss: 1.118\n",
      "Epoch: 6 Progress: 28% Loss: 1.217\n",
      "Epoch: 6 Progress: 32% Loss: 1.388\n",
      "Epoch: 6 Progress: 36% Loss: 1.326\n",
      "Epoch: 6 Progress: 40% Loss: 1.368\n",
      "Epoch: 6 Progress: 43% Loss: 1.305\n",
      "Epoch: 6 Progress: 47% Loss: 1.248\n",
      "Epoch: 6 Progress: 51% Loss: 1.189\n",
      "Epoch: 6 Progress: 55% Loss: 1.180\n",
      "Epoch: 6 Progress: 59% Loss: 1.170\n",
      "Epoch: 6 Progress: 63% Loss: 1.130\n",
      "Epoch: 6 Progress: 67% Loss: 1.174\n",
      "Epoch: 6 Progress: 71% Loss: 1.214\n",
      "Epoch: 6 Progress: 75% Loss: 1.209\n",
      "Epoch: 6 Progress: 79% Loss: 1.319\n",
      "Epoch: 6 Progress: 83% Loss: 1.326\n",
      "Epoch: 6 Progress: 87% Loss: 1.307\n",
      "Epoch: 6 Progress: 91% Loss: 1.290\n",
      "Epoch: 6 Progress: 95% Loss: 1.285\n",
      "Epoch: 6 Progress: 99% Loss: 1.367\n",
      "\n",
      "Epoch: 6 Loss: tensor(1.3694)\n",
      "\n",
      "Epoch: 7 Progress:  0% Loss: 0.018\n",
      "Epoch: 7 Progress:  4% Loss: 0.614\n",
      "Epoch: 7 Progress:  8% Loss: 1.847\n",
      "Epoch: 7 Progress: 12% Loss: 1.400\n",
      "Epoch: 7 Progress: 16% Loss: 1.158\n",
      "Epoch: 7 Progress: 20% Loss: 1.019\n",
      "Epoch: 7 Progress: 24% Loss: 1.116\n",
      "Epoch: 7 Progress: 28% Loss: 1.200\n",
      "Epoch: 7 Progress: 32% Loss: 1.359\n",
      "Epoch: 7 Progress: 36% Loss: 1.295\n",
      "Epoch: 7 Progress: 40% Loss: 1.337\n",
      "Epoch: 7 Progress: 43% Loss: 1.276\n",
      "Epoch: 7 Progress: 47% Loss: 1.220\n",
      "Epoch: 7 Progress: 51% Loss: 1.161\n",
      "Epoch: 7 Progress: 55% Loss: 1.147\n",
      "Epoch: 7 Progress: 59% Loss: 1.134\n",
      "Epoch: 7 Progress: 63% Loss: 1.096\n",
      "Epoch: 7 Progress: 67% Loss: 1.139\n",
      "Epoch: 7 Progress: 71% Loss: 1.175\n",
      "Epoch: 7 Progress: 75% Loss: 1.167\n",
      "Epoch: 7 Progress: 79% Loss: 1.267\n",
      "Epoch: 7 Progress: 83% Loss: 1.279\n",
      "Epoch: 7 Progress: 87% Loss: 1.259\n",
      "Epoch: 7 Progress: 91% Loss: 1.246\n",
      "Epoch: 7 Progress: 95% Loss: 1.240\n",
      "Epoch: 7 Progress: 99% Loss: 1.330\n",
      "\n",
      "Epoch: 7 Loss: tensor(1.3322)\n",
      "\n",
      "Epoch: 8 Progress:  0% Loss: 0.027\n",
      "Epoch: 8 Progress:  4% Loss: 0.612\n",
      "Epoch: 8 Progress:  8% Loss: 1.851\n",
      "Epoch: 8 Progress: 12% Loss: 1.394\n",
      "Epoch: 8 Progress: 16% Loss: 1.158\n",
      "Epoch: 8 Progress: 20% Loss: 1.014\n",
      "Epoch: 8 Progress: 24% Loss: 1.119\n",
      "Epoch: 8 Progress: 28% Loss: 1.195\n",
      "Epoch: 8 Progress: 32% Loss: 1.368\n",
      "Epoch: 8 Progress: 36% Loss: 1.308\n",
      "Epoch: 8 Progress: 40% Loss: 1.355\n",
      "Epoch: 8 Progress: 43% Loss: 1.292\n",
      "Epoch: 8 Progress: 47% Loss: 1.235\n",
      "Epoch: 8 Progress: 51% Loss: 1.173\n",
      "Epoch: 8 Progress: 55% Loss: 1.162\n",
      "Epoch: 8 Progress: 59% Loss: 1.151\n",
      "Epoch: 8 Progress: 63% Loss: 1.111\n",
      "Epoch: 8 Progress: 67% Loss: 1.156\n",
      "Epoch: 8 Progress: 71% Loss: 1.198\n",
      "Epoch: 8 Progress: 75% Loss: 1.194\n",
      "Epoch: 8 Progress: 79% Loss: 1.294\n",
      "Epoch: 8 Progress: 83% Loss: 1.305\n",
      "Epoch: 8 Progress: 87% Loss: 1.283\n",
      "Epoch: 8 Progress: 91% Loss: 1.268\n",
      "Epoch: 8 Progress: 95% Loss: 1.262\n",
      "Epoch: 8 Progress: 99% Loss: 1.341\n",
      "\n",
      "Epoch: 8 Loss: tensor(1.3427)\n",
      "\n",
      "Epoch: 9 Progress:  0% Loss: 0.126\n",
      "Epoch: 9 Progress:  4% Loss: 0.615\n",
      "Epoch: 9 Progress:  8% Loss: 1.832\n",
      "Epoch: 9 Progress: 12% Loss: 1.387\n",
      "Epoch: 9 Progress: 16% Loss: 1.149\n",
      "Epoch: 9 Progress: 20% Loss: 1.008\n",
      "Epoch: 9 Progress: 24% Loss: 1.105\n",
      "Epoch: 9 Progress: 28% Loss: 1.200\n",
      "Epoch: 9 Progress: 32% Loss: 1.360\n",
      "Epoch: 9 Progress: 36% Loss: 1.298\n",
      "Epoch: 9 Progress: 40% Loss: 1.344\n",
      "Epoch: 9 Progress: 43% Loss: 1.280\n",
      "Epoch: 9 Progress: 47% Loss: 1.226\n",
      "Epoch: 9 Progress: 51% Loss: 1.165\n",
      "Epoch: 9 Progress: 55% Loss: 1.154\n",
      "Epoch: 9 Progress: 59% Loss: 1.143\n",
      "Epoch: 9 Progress: 63% Loss: 1.103\n",
      "Epoch: 9 Progress: 67% Loss: 1.148\n",
      "Epoch: 9 Progress: 71% Loss: 1.182\n",
      "Epoch: 9 Progress: 75% Loss: 1.175\n",
      "Epoch: 9 Progress: 79% Loss: 1.269\n",
      "Epoch: 9 Progress: 83% Loss: 1.277\n",
      "Epoch: 9 Progress: 87% Loss: 1.257\n",
      "Epoch: 9 Progress: 91% Loss: 1.240\n",
      "Epoch: 9 Progress: 95% Loss: 1.240\n",
      "Epoch: 9 Progress: 99% Loss: 1.322\n",
      "\n",
      "Epoch: 9 Loss: tensor(1.3227)\n",
      "\n",
      "Epoch: 10 Progress:  0% Loss: 0.065\n",
      "Epoch: 10 Progress:  4% Loss: 0.600\n",
      "Epoch: 10 Progress:  8% Loss: 1.782\n",
      "Epoch: 10 Progress: 12% Loss: 1.360\n",
      "Epoch: 10 Progress: 16% Loss: 1.130\n",
      "Epoch: 10 Progress: 20% Loss: 0.989\n",
      "Epoch: 10 Progress: 24% Loss: 1.070\n",
      "Epoch: 10 Progress: 28% Loss: 1.177\n",
      "Epoch: 10 Progress: 32% Loss: 1.321\n",
      "Epoch: 10 Progress: 36% Loss: 1.264\n",
      "Epoch: 10 Progress: 40% Loss: 1.312\n",
      "Epoch: 10 Progress: 43% Loss: 1.251\n",
      "Epoch: 10 Progress: 47% Loss: 1.198\n",
      "Epoch: 10 Progress: 51% Loss: 1.140\n",
      "Epoch: 10 Progress: 55% Loss: 1.129\n",
      "Epoch: 10 Progress: 59% Loss: 1.114\n",
      "Epoch: 10 Progress: 63% Loss: 1.078\n",
      "Epoch: 10 Progress: 67% Loss: 1.120\n",
      "Epoch: 10 Progress: 71% Loss: 1.154\n",
      "Epoch: 10 Progress: 75% Loss: 1.147\n",
      "Epoch: 10 Progress: 79% Loss: 1.242\n",
      "Epoch: 10 Progress: 83% Loss: 1.252\n",
      "Epoch: 10 Progress: 87% Loss: 1.232\n",
      "Epoch: 10 Progress: 91% Loss: 1.216\n",
      "Epoch: 10 Progress: 95% Loss: 1.213\n",
      "Epoch: 10 Progress: 99% Loss: 1.290\n",
      "\n",
      "Epoch: 10 Loss: tensor(1.2942)\n",
      "\n",
      "Epoch: 11 Progress:  0% Loss: 0.064\n",
      "Epoch: 11 Progress:  4% Loss: 0.624\n",
      "Epoch: 11 Progress:  8% Loss: 1.832\n",
      "Epoch: 11 Progress: 12% Loss: 1.385\n",
      "Epoch: 11 Progress: 16% Loss: 1.156\n",
      "Epoch: 11 Progress: 20% Loss: 1.014\n",
      "Epoch: 11 Progress: 24% Loss: 1.097\n",
      "Epoch: 11 Progress: 28% Loss: 1.202\n",
      "Epoch: 11 Progress: 32% Loss: 1.362\n",
      "Epoch: 11 Progress: 36% Loss: 1.299\n",
      "Epoch: 11 Progress: 40% Loss: 1.343\n",
      "Epoch: 11 Progress: 43% Loss: 1.278\n",
      "Epoch: 11 Progress: 47% Loss: 1.221\n",
      "Epoch: 11 Progress: 51% Loss: 1.163\n",
      "Epoch: 11 Progress: 55% Loss: 1.147\n",
      "Epoch: 11 Progress: 59% Loss: 1.130\n",
      "Epoch: 11 Progress: 63% Loss: 1.092\n",
      "Epoch: 11 Progress: 67% Loss: 1.128\n",
      "Epoch: 11 Progress: 71% Loss: 1.165\n",
      "Epoch: 11 Progress: 75% Loss: 1.153\n",
      "Epoch: 11 Progress: 79% Loss: 1.256\n",
      "Epoch: 11 Progress: 83% Loss: 1.267\n",
      "Epoch: 11 Progress: 87% Loss: 1.246\n",
      "Epoch: 11 Progress: 91% Loss: 1.229\n",
      "Epoch: 11 Progress: 95% Loss: 1.227\n",
      "Epoch: 11 Progress: 99% Loss: 1.305\n",
      "\n",
      "Epoch: 11 Loss: tensor(1.3063)\n",
      "\n",
      "Epoch: 12 Progress:  0% Loss: 0.108\n",
      "Epoch: 12 Progress:  4% Loss: 0.596\n",
      "Epoch: 12 Progress:  8% Loss: 1.837\n",
      "Epoch: 12 Progress: 12% Loss: 1.383\n",
      "Epoch: 12 Progress: 16% Loss: 1.142\n",
      "Epoch: 12 Progress: 20% Loss: 1.003\n",
      "Epoch: 12 Progress: 24% Loss: 1.084\n",
      "Epoch: 12 Progress: 28% Loss: 1.175\n",
      "Epoch: 12 Progress: 32% Loss: 1.331\n",
      "Epoch: 12 Progress: 36% Loss: 1.272\n",
      "Epoch: 12 Progress: 40% Loss: 1.315\n",
      "Epoch: 12 Progress: 43% Loss: 1.251\n",
      "Epoch: 12 Progress: 47% Loss: 1.194\n",
      "Epoch: 12 Progress: 51% Loss: 1.136\n",
      "Epoch: 12 Progress: 55% Loss: 1.122\n",
      "Epoch: 12 Progress: 59% Loss: 1.110\n",
      "Epoch: 12 Progress: 63% Loss: 1.072\n",
      "Epoch: 12 Progress: 67% Loss: 1.111\n",
      "Epoch: 12 Progress: 71% Loss: 1.150\n",
      "Epoch: 12 Progress: 75% Loss: 1.144\n",
      "Epoch: 12 Progress: 79% Loss: 1.246\n",
      "Epoch: 12 Progress: 83% Loss: 1.256\n",
      "Epoch: 12 Progress: 87% Loss: 1.236\n",
      "Epoch: 12 Progress: 91% Loss: 1.220\n",
      "Epoch: 12 Progress: 95% Loss: 1.216\n",
      "Epoch: 12 Progress: 99% Loss: 1.294\n",
      "\n",
      "Epoch: 12 Loss: tensor(1.2954)\n",
      "\n",
      "Epoch: 13 Progress:  0% Loss: 0.015\n",
      "Epoch: 13 Progress:  4% Loss: 0.581\n",
      "Epoch: 13 Progress:  8% Loss: 1.832\n",
      "Epoch: 13 Progress: 12% Loss: 1.371\n",
      "Epoch: 13 Progress: 16% Loss: 1.138\n",
      "Epoch: 13 Progress: 20% Loss: 1.000\n",
      "Epoch: 13 Progress: 24% Loss: 1.096\n",
      "Epoch: 13 Progress: 28% Loss: 1.180\n",
      "Epoch: 13 Progress: 32% Loss: 1.333\n",
      "Epoch: 13 Progress: 36% Loss: 1.276\n",
      "Epoch: 13 Progress: 40% Loss: 1.322\n",
      "Epoch: 13 Progress: 43% Loss: 1.256\n",
      "Epoch: 13 Progress: 47% Loss: 1.201\n",
      "Epoch: 13 Progress: 51% Loss: 1.143\n",
      "Epoch: 13 Progress: 55% Loss: 1.125\n",
      "Epoch: 13 Progress: 59% Loss: 1.109\n",
      "Epoch: 13 Progress: 63% Loss: 1.071\n",
      "Epoch: 13 Progress: 67% Loss: 1.109\n",
      "Epoch: 13 Progress: 71% Loss: 1.141\n",
      "Epoch: 13 Progress: 75% Loss: 1.132\n",
      "Epoch: 13 Progress: 79% Loss: 1.226\n",
      "Epoch: 13 Progress: 83% Loss: 1.235\n",
      "Epoch: 13 Progress: 87% Loss: 1.216\n",
      "Epoch: 13 Progress: 91% Loss: 1.199\n",
      "Epoch: 13 Progress: 95% Loss: 1.198\n",
      "Epoch: 13 Progress: 99% Loss: 1.272\n",
      "\n",
      "Epoch: 13 Loss: tensor(1.2745)\n",
      "\n",
      "Epoch: 14 Progress:  0% Loss: 0.156\n",
      "Epoch: 14 Progress:  4% Loss: 0.603\n",
      "Epoch: 14 Progress:  8% Loss: 1.798\n",
      "Epoch: 14 Progress: 12% Loss: 1.355\n",
      "Epoch: 14 Progress: 16% Loss: 1.124\n",
      "Epoch: 14 Progress: 20% Loss: 0.987\n",
      "Epoch: 14 Progress: 24% Loss: 1.076\n",
      "Epoch: 14 Progress: 28% Loss: 1.162\n",
      "Epoch: 14 Progress: 32% Loss: 1.319\n",
      "Epoch: 14 Progress: 36% Loss: 1.259\n",
      "Epoch: 14 Progress: 40% Loss: 1.304\n",
      "Epoch: 14 Progress: 43% Loss: 1.240\n",
      "Epoch: 14 Progress: 47% Loss: 1.185\n",
      "Epoch: 14 Progress: 51% Loss: 1.128\n",
      "Epoch: 14 Progress: 55% Loss: 1.113\n",
      "Epoch: 14 Progress: 59% Loss: 1.099\n",
      "Epoch: 14 Progress: 63% Loss: 1.062\n",
      "Epoch: 14 Progress: 67% Loss: 1.094\n",
      "Epoch: 14 Progress: 71% Loss: 1.130\n",
      "Epoch: 14 Progress: 75% Loss: 1.122\n",
      "Epoch: 14 Progress: 79% Loss: 1.220\n",
      "Epoch: 14 Progress: 83% Loss: 1.231\n",
      "Epoch: 14 Progress: 87% Loss: 1.209\n",
      "Epoch: 14 Progress: 91% Loss: 1.193\n",
      "Epoch: 14 Progress: 95% Loss: 1.191\n",
      "Epoch: 14 Progress: 99% Loss: 1.264\n",
      "\n",
      "Epoch: 14 Loss: tensor(1.2658)\n",
      "\n",
      "Epoch: 15 Progress:  0% Loss: 0.118\n",
      "Epoch: 15 Progress:  4% Loss: 0.621\n",
      "Epoch: 15 Progress:  8% Loss: 1.751\n",
      "Epoch: 15 Progress: 12% Loss: 1.327\n",
      "Epoch: 15 Progress: 16% Loss: 1.103\n",
      "Epoch: 15 Progress: 20% Loss: 0.967\n",
      "Epoch: 15 Progress: 24% Loss: 1.063\n",
      "Epoch: 15 Progress: 28% Loss: 1.145\n",
      "Epoch: 15 Progress: 32% Loss: 1.299\n",
      "Epoch: 15 Progress: 36% Loss: 1.245\n",
      "Epoch: 15 Progress: 40% Loss: 1.287\n",
      "Epoch: 15 Progress: 43% Loss: 1.225\n",
      "Epoch: 15 Progress: 47% Loss: 1.173\n",
      "Epoch: 15 Progress: 51% Loss: 1.117\n",
      "Epoch: 15 Progress: 55% Loss: 1.109\n",
      "Epoch: 15 Progress: 59% Loss: 1.094\n",
      "Epoch: 15 Progress: 63% Loss: 1.057\n",
      "Epoch: 15 Progress: 67% Loss: 1.090\n",
      "Epoch: 15 Progress: 71% Loss: 1.121\n",
      "Epoch: 15 Progress: 75% Loss: 1.114\n",
      "Epoch: 15 Progress: 79% Loss: 1.211\n",
      "Epoch: 15 Progress: 83% Loss: 1.223\n",
      "Epoch: 15 Progress: 87% Loss: 1.204\n",
      "Epoch: 15 Progress: 91% Loss: 1.190\n",
      "Epoch: 15 Progress: 95% Loss: 1.189\n",
      "Epoch: 15 Progress: 99% Loss: 1.264\n",
      "\n",
      "Epoch: 15 Loss: tensor(1.2648)\n",
      "\n",
      "Epoch: 16 Progress:  0% Loss: 0.024\n",
      "Epoch: 16 Progress:  4% Loss: 0.596\n",
      "Epoch: 16 Progress:  8% Loss: 1.798\n",
      "Epoch: 16 Progress: 12% Loss: 1.359\n",
      "Epoch: 16 Progress: 16% Loss: 1.128\n",
      "Epoch: 16 Progress: 20% Loss: 0.985\n",
      "Epoch: 16 Progress: 24% Loss: 1.072\n",
      "Epoch: 16 Progress: 28% Loss: 1.159\n",
      "Epoch: 16 Progress: 32% Loss: 1.301\n",
      "Epoch: 16 Progress: 36% Loss: 1.243\n",
      "Epoch: 16 Progress: 40% Loss: 1.282\n",
      "Epoch: 16 Progress: 43% Loss: 1.219\n",
      "Epoch: 16 Progress: 47% Loss: 1.166\n",
      "Epoch: 16 Progress: 51% Loss: 1.110\n",
      "Epoch: 16 Progress: 55% Loss: 1.094\n",
      "Epoch: 16 Progress: 59% Loss: 1.082\n",
      "Epoch: 16 Progress: 63% Loss: 1.044\n",
      "Epoch: 16 Progress: 67% Loss: 1.072\n",
      "Epoch: 16 Progress: 71% Loss: 1.104\n",
      "Epoch: 16 Progress: 75% Loss: 1.096\n",
      "Epoch: 16 Progress: 79% Loss: 1.191\n",
      "Epoch: 16 Progress: 83% Loss: 1.200\n",
      "Epoch: 16 Progress: 87% Loss: 1.183\n",
      "Epoch: 16 Progress: 91% Loss: 1.166\n",
      "Epoch: 16 Progress: 95% Loss: 1.165\n",
      "Epoch: 16 Progress: 99% Loss: 1.235\n",
      "\n",
      "Epoch: 16 Loss: tensor(1.2357)\n",
      "\n",
      "Epoch: 17 Progress:  0% Loss: 0.328\n",
      "Epoch: 17 Progress:  4% Loss: 0.599\n",
      "Epoch: 17 Progress:  8% Loss: 1.737\n",
      "Epoch: 17 Progress: 12% Loss: 1.321\n",
      "Epoch: 17 Progress: 16% Loss: 1.095\n",
      "Epoch: 17 Progress: 20% Loss: 0.957\n",
      "Epoch: 17 Progress: 24% Loss: 1.054\n",
      "Epoch: 17 Progress: 28% Loss: 1.144\n",
      "Epoch: 17 Progress: 32% Loss: 1.287\n",
      "Epoch: 17 Progress: 36% Loss: 1.227\n",
      "Epoch: 17 Progress: 40% Loss: 1.273\n",
      "Epoch: 17 Progress: 43% Loss: 1.209\n",
      "Epoch: 17 Progress: 47% Loss: 1.158\n",
      "Epoch: 17 Progress: 51% Loss: 1.103\n",
      "Epoch: 17 Progress: 55% Loss: 1.088\n",
      "Epoch: 17 Progress: 59% Loss: 1.074\n",
      "Epoch: 17 Progress: 63% Loss: 1.037\n",
      "Epoch: 17 Progress: 67% Loss: 1.067\n",
      "Epoch: 17 Progress: 71% Loss: 1.095\n",
      "Epoch: 17 Progress: 75% Loss: 1.088\n",
      "Epoch: 17 Progress: 79% Loss: 1.187\n",
      "Epoch: 17 Progress: 83% Loss: 1.196\n",
      "Epoch: 17 Progress: 87% Loss: 1.177\n",
      "Epoch: 17 Progress: 91% Loss: 1.162\n",
      "Epoch: 17 Progress: 95% Loss: 1.160\n",
      "Epoch: 17 Progress: 99% Loss: 1.236\n",
      "\n",
      "Epoch: 17 Loss: tensor(1.2374)\n",
      "\n",
      "Epoch: 18 Progress:  0% Loss: 0.170\n",
      "Epoch: 18 Progress:  4% Loss: 0.591\n",
      "Epoch: 18 Progress:  8% Loss: 1.786\n",
      "Epoch: 18 Progress: 12% Loss: 1.343\n",
      "Epoch: 18 Progress: 16% Loss: 1.113\n",
      "Epoch: 18 Progress: 20% Loss: 0.975\n",
      "Epoch: 18 Progress: 24% Loss: 1.054\n",
      "Epoch: 18 Progress: 28% Loss: 1.137\n",
      "Epoch: 18 Progress: 32% Loss: 1.288\n",
      "Epoch: 18 Progress: 36% Loss: 1.227\n",
      "Epoch: 18 Progress: 40% Loss: 1.270\n",
      "Epoch: 18 Progress: 43% Loss: 1.209\n",
      "Epoch: 18 Progress: 47% Loss: 1.156\n",
      "Epoch: 18 Progress: 51% Loss: 1.100\n",
      "Epoch: 18 Progress: 55% Loss: 1.081\n",
      "Epoch: 18 Progress: 59% Loss: 1.068\n",
      "Epoch: 18 Progress: 63% Loss: 1.032\n",
      "Epoch: 18 Progress: 67% Loss: 1.065\n",
      "Epoch: 18 Progress: 71% Loss: 1.089\n",
      "Epoch: 18 Progress: 75% Loss: 1.081\n",
      "Epoch: 18 Progress: 79% Loss: 1.174\n",
      "Epoch: 18 Progress: 83% Loss: 1.184\n",
      "Epoch: 18 Progress: 87% Loss: 1.166\n",
      "Epoch: 18 Progress: 91% Loss: 1.150\n",
      "Epoch: 18 Progress: 95% Loss: 1.148\n",
      "Epoch: 18 Progress: 99% Loss: 1.220\n",
      "\n",
      "Epoch: 18 Loss: tensor(1.2213)\n",
      "\n",
      "Epoch: 19 Progress:  0% Loss: 0.025\n",
      "Epoch: 19 Progress:  4% Loss: 0.584\n",
      "Epoch: 19 Progress:  8% Loss: 1.751\n",
      "Epoch: 19 Progress: 12% Loss: 1.320\n",
      "Epoch: 19 Progress: 16% Loss: 1.092\n",
      "Epoch: 19 Progress: 20% Loss: 0.953\n",
      "Epoch: 19 Progress: 24% Loss: 1.037\n",
      "Epoch: 19 Progress: 28% Loss: 1.127\n",
      "Epoch: 19 Progress: 32% Loss: 1.273\n",
      "Epoch: 19 Progress: 36% Loss: 1.212\n",
      "Epoch: 19 Progress: 40% Loss: 1.258\n",
      "Epoch: 19 Progress: 43% Loss: 1.199\n",
      "Epoch: 19 Progress: 47% Loss: 1.147\n",
      "Epoch: 19 Progress: 51% Loss: 1.092\n",
      "Epoch: 19 Progress: 55% Loss: 1.076\n",
      "Epoch: 19 Progress: 59% Loss: 1.063\n",
      "Epoch: 19 Progress: 63% Loss: 1.026\n",
      "Epoch: 19 Progress: 67% Loss: 1.062\n",
      "Epoch: 19 Progress: 71% Loss: 1.091\n",
      "Epoch: 19 Progress: 75% Loss: 1.084\n",
      "Epoch: 19 Progress: 79% Loss: 1.177\n",
      "Epoch: 19 Progress: 83% Loss: 1.188\n",
      "Epoch: 19 Progress: 87% Loss: 1.169\n",
      "Epoch: 19 Progress: 91% Loss: 1.153\n",
      "Epoch: 19 Progress: 95% Loss: 1.151\n",
      "Epoch: 19 Progress: 99% Loss: 1.223\n",
      "\n",
      "Epoch: 19 Loss: tensor(1.2244)\n",
      "\n",
      "Epoch: 20 Progress:  0% Loss: 0.058\n",
      "Epoch: 20 Progress:  4% Loss: 0.591\n",
      "Epoch: 20 Progress:  8% Loss: 1.738\n",
      "Epoch: 20 Progress: 12% Loss: 1.313\n",
      "Epoch: 20 Progress: 16% Loss: 1.090\n",
      "Epoch: 20 Progress: 20% Loss: 0.955\n",
      "Epoch: 20 Progress: 24% Loss: 1.041\n",
      "Epoch: 20 Progress: 28% Loss: 1.131\n",
      "Epoch: 20 Progress: 32% Loss: 1.278\n",
      "Epoch: 20 Progress: 36% Loss: 1.218\n",
      "Epoch: 20 Progress: 40% Loss: 1.261\n",
      "Epoch: 20 Progress: 43% Loss: 1.200\n",
      "Epoch: 20 Progress: 47% Loss: 1.148\n",
      "Epoch: 20 Progress: 51% Loss: 1.093\n",
      "Epoch: 20 Progress: 55% Loss: 1.075\n",
      "Epoch: 20 Progress: 59% Loss: 1.063\n",
      "Epoch: 20 Progress: 63% Loss: 1.026\n",
      "Epoch: 20 Progress: 67% Loss: 1.059\n",
      "Epoch: 20 Progress: 71% Loss: 1.088\n",
      "Epoch: 20 Progress: 75% Loss: 1.082\n",
      "Epoch: 20 Progress: 79% Loss: 1.175\n",
      "Epoch: 20 Progress: 83% Loss: 1.185\n",
      "Epoch: 20 Progress: 87% Loss: 1.166\n",
      "Epoch: 20 Progress: 91% Loss: 1.151\n",
      "Epoch: 20 Progress: 95% Loss: 1.151\n",
      "Epoch: 20 Progress: 99% Loss: 1.224\n",
      "\n",
      "Epoch: 20 Loss: tensor(1.2249)\n",
      "\n",
      "Epoch: 21 Progress:  0% Loss: 0.048\n",
      "Epoch: 21 Progress:  4% Loss: 0.587\n",
      "Epoch: 21 Progress:  8% Loss: 1.706\n",
      "Epoch: 21 Progress: 12% Loss: 1.294\n",
      "Epoch: 21 Progress: 16% Loss: 1.075\n",
      "Epoch: 21 Progress: 20% Loss: 0.943\n",
      "Epoch: 21 Progress: 24% Loss: 1.028\n",
      "Epoch: 21 Progress: 28% Loss: 1.112\n",
      "Epoch: 21 Progress: 32% Loss: 1.254\n",
      "Epoch: 21 Progress: 36% Loss: 1.199\n",
      "Epoch: 21 Progress: 40% Loss: 1.248\n",
      "Epoch: 21 Progress: 43% Loss: 1.189\n",
      "Epoch: 21 Progress: 47% Loss: 1.138\n",
      "Epoch: 21 Progress: 51% Loss: 1.084\n",
      "Epoch: 21 Progress: 55% Loss: 1.069\n",
      "Epoch: 21 Progress: 59% Loss: 1.053\n",
      "Epoch: 21 Progress: 63% Loss: 1.018\n",
      "Epoch: 21 Progress: 67% Loss: 1.052\n",
      "Epoch: 21 Progress: 71% Loss: 1.081\n",
      "Epoch: 21 Progress: 75% Loss: 1.073\n",
      "Epoch: 21 Progress: 79% Loss: 1.167\n",
      "Epoch: 21 Progress: 83% Loss: 1.176\n",
      "Epoch: 21 Progress: 87% Loss: 1.158\n",
      "Epoch: 21 Progress: 91% Loss: 1.143\n",
      "Epoch: 21 Progress: 95% Loss: 1.142\n",
      "Epoch: 21 Progress: 99% Loss: 1.210\n",
      "\n",
      "Epoch: 21 Loss: tensor(1.2116)\n",
      "\n",
      "Epoch: 22 Progress:  0% Loss: 0.000\n",
      "Epoch: 22 Progress:  4% Loss: 0.588\n",
      "Epoch: 22 Progress:  8% Loss: 1.710\n",
      "Epoch: 22 Progress: 12% Loss: 1.313\n",
      "Epoch: 22 Progress: 16% Loss: 1.091\n",
      "Epoch: 22 Progress: 20% Loss: 0.954\n",
      "Epoch: 22 Progress: 24% Loss: 1.039\n",
      "Epoch: 22 Progress: 28% Loss: 1.120\n",
      "Epoch: 22 Progress: 32% Loss: 1.264\n",
      "Epoch: 22 Progress: 36% Loss: 1.203\n",
      "Epoch: 22 Progress: 40% Loss: 1.248\n",
      "Epoch: 22 Progress: 43% Loss: 1.188\n",
      "Epoch: 22 Progress: 47% Loss: 1.136\n",
      "Epoch: 22 Progress: 51% Loss: 1.082\n",
      "Epoch: 22 Progress: 55% Loss: 1.065\n",
      "Epoch: 22 Progress: 59% Loss: 1.052\n",
      "Epoch: 22 Progress: 63% Loss: 1.014\n",
      "Epoch: 22 Progress: 67% Loss: 1.047\n",
      "Epoch: 22 Progress: 71% Loss: 1.071\n",
      "Epoch: 22 Progress: 75% Loss: 1.061\n",
      "Epoch: 22 Progress: 79% Loss: 1.157\n",
      "Epoch: 22 Progress: 83% Loss: 1.167\n",
      "Epoch: 22 Progress: 87% Loss: 1.149\n",
      "Epoch: 22 Progress: 91% Loss: 1.134\n",
      "Epoch: 22 Progress: 95% Loss: 1.135\n",
      "Epoch: 22 Progress: 99% Loss: 1.201\n",
      "\n",
      "Epoch: 22 Loss: tensor(1.2028)\n",
      "\n",
      "Epoch: 23 Progress:  0% Loss: 0.056\n",
      "Epoch: 23 Progress:  4% Loss: 0.584\n",
      "Epoch: 23 Progress:  8% Loss: 1.731\n",
      "Epoch: 23 Progress: 12% Loss: 1.303\n",
      "Epoch: 23 Progress: 16% Loss: 1.082\n",
      "Epoch: 23 Progress: 20% Loss: 0.950\n",
      "Epoch: 23 Progress: 24% Loss: 1.030\n",
      "Epoch: 23 Progress: 28% Loss: 1.116\n",
      "Epoch: 23 Progress: 32% Loss: 1.257\n",
      "Epoch: 23 Progress: 36% Loss: 1.199\n",
      "Epoch: 23 Progress: 40% Loss: 1.242\n",
      "Epoch: 23 Progress: 43% Loss: 1.184\n",
      "Epoch: 23 Progress: 47% Loss: 1.134\n",
      "Epoch: 23 Progress: 51% Loss: 1.080\n",
      "Epoch: 23 Progress: 55% Loss: 1.063\n",
      "Epoch: 23 Progress: 59% Loss: 1.048\n",
      "Epoch: 23 Progress: 63% Loss: 1.011\n",
      "Epoch: 23 Progress: 67% Loss: 1.046\n",
      "Epoch: 23 Progress: 71% Loss: 1.070\n",
      "Epoch: 23 Progress: 75% Loss: 1.061\n",
      "Epoch: 23 Progress: 79% Loss: 1.155\n",
      "Epoch: 23 Progress: 83% Loss: 1.164\n",
      "Epoch: 23 Progress: 87% Loss: 1.147\n",
      "Epoch: 23 Progress: 91% Loss: 1.132\n",
      "Epoch: 23 Progress: 95% Loss: 1.133\n",
      "Epoch: 23 Progress: 99% Loss: 1.205\n",
      "\n",
      "Epoch: 23 Loss: tensor(1.2057)\n",
      "\n",
      "Epoch: 24 Progress:  0% Loss: 0.042\n",
      "Epoch: 24 Progress:  4% Loss: 0.592\n",
      "Epoch: 24 Progress:  8% Loss: 1.744\n",
      "Epoch: 24 Progress: 12% Loss: 1.307\n",
      "Epoch: 24 Progress: 16% Loss: 1.085\n",
      "Epoch: 24 Progress: 20% Loss: 0.950\n",
      "Epoch: 24 Progress: 24% Loss: 1.031\n",
      "Epoch: 24 Progress: 28% Loss: 1.116\n",
      "Epoch: 24 Progress: 32% Loss: 1.257\n",
      "Epoch: 24 Progress: 36% Loss: 1.199\n",
      "Epoch: 24 Progress: 40% Loss: 1.240\n",
      "Epoch: 24 Progress: 43% Loss: 1.181\n",
      "Epoch: 24 Progress: 47% Loss: 1.131\n",
      "Epoch: 24 Progress: 51% Loss: 1.076\n",
      "Epoch: 24 Progress: 55% Loss: 1.061\n",
      "Epoch: 24 Progress: 59% Loss: 1.047\n",
      "Epoch: 24 Progress: 63% Loss: 1.011\n",
      "Epoch: 24 Progress: 67% Loss: 1.040\n",
      "Epoch: 24 Progress: 71% Loss: 1.067\n",
      "Epoch: 24 Progress: 75% Loss: 1.058\n",
      "Epoch: 24 Progress: 79% Loss: 1.152\n",
      "Epoch: 24 Progress: 83% Loss: 1.162\n",
      "Epoch: 24 Progress: 87% Loss: 1.145\n",
      "Epoch: 24 Progress: 91% Loss: 1.129\n",
      "Epoch: 24 Progress: 95% Loss: 1.129\n",
      "Epoch: 24 Progress: 99% Loss: 1.200\n",
      "\n",
      "Epoch: 24 Loss: tensor(1.2018)\n",
      "\n",
      "Epoch: 25 Progress:  0% Loss: 0.006\n",
      "Epoch: 25 Progress:  4% Loss: 0.567\n",
      "Epoch: 25 Progress:  8% Loss: 1.706\n",
      "Epoch: 25 Progress: 12% Loss: 1.291\n",
      "Epoch: 25 Progress: 16% Loss: 1.073\n",
      "Epoch: 25 Progress: 20% Loss: 0.940\n",
      "Epoch: 25 Progress: 24% Loss: 1.030\n",
      "Epoch: 25 Progress: 28% Loss: 1.108\n",
      "Epoch: 25 Progress: 32% Loss: 1.247\n",
      "Epoch: 25 Progress: 36% Loss: 1.193\n",
      "Epoch: 25 Progress: 40% Loss: 1.232\n",
      "Epoch: 25 Progress: 43% Loss: 1.174\n",
      "Epoch: 25 Progress: 47% Loss: 1.122\n",
      "Epoch: 25 Progress: 51% Loss: 1.069\n",
      "Epoch: 25 Progress: 55% Loss: 1.052\n",
      "Epoch: 25 Progress: 59% Loss: 1.040\n",
      "Epoch: 25 Progress: 63% Loss: 1.004\n",
      "Epoch: 25 Progress: 67% Loss: 1.033\n",
      "Epoch: 25 Progress: 71% Loss: 1.062\n",
      "Epoch: 25 Progress: 75% Loss: 1.053\n",
      "Epoch: 25 Progress: 79% Loss: 1.148\n",
      "Epoch: 25 Progress: 83% Loss: 1.155\n",
      "Epoch: 25 Progress: 87% Loss: 1.138\n",
      "Epoch: 25 Progress: 91% Loss: 1.123\n",
      "Epoch: 25 Progress: 95% Loss: 1.123\n",
      "Epoch: 25 Progress: 99% Loss: 1.190\n",
      "\n",
      "Epoch: 25 Loss: tensor(1.1917)\n",
      "\n",
      "Epoch: 26 Progress:  0% Loss: 0.086\n",
      "Epoch: 26 Progress:  4% Loss: 0.568\n",
      "Epoch: 26 Progress:  8% Loss: 1.700\n",
      "Epoch: 26 Progress: 12% Loss: 1.295\n",
      "Epoch: 26 Progress: 16% Loss: 1.072\n",
      "Epoch: 26 Progress: 20% Loss: 0.941\n",
      "Epoch: 26 Progress: 24% Loss: 1.018\n",
      "Epoch: 26 Progress: 28% Loss: 1.099\n",
      "Epoch: 26 Progress: 32% Loss: 1.240\n",
      "Epoch: 26 Progress: 36% Loss: 1.185\n",
      "Epoch: 26 Progress: 40% Loss: 1.222\n",
      "Epoch: 26 Progress: 43% Loss: 1.165\n",
      "Epoch: 26 Progress: 47% Loss: 1.113\n",
      "Epoch: 26 Progress: 51% Loss: 1.060\n",
      "Epoch: 26 Progress: 55% Loss: 1.043\n",
      "Epoch: 26 Progress: 59% Loss: 1.032\n",
      "Epoch: 26 Progress: 63% Loss: 0.996\n",
      "Epoch: 26 Progress: 67% Loss: 1.023\n",
      "Epoch: 26 Progress: 71% Loss: 1.052\n",
      "Epoch: 26 Progress: 75% Loss: 1.044\n",
      "Epoch: 26 Progress: 79% Loss: 1.142\n",
      "Epoch: 26 Progress: 83% Loss: 1.151\n",
      "Epoch: 26 Progress: 87% Loss: 1.133\n",
      "Epoch: 26 Progress: 91% Loss: 1.118\n",
      "Epoch: 26 Progress: 95% Loss: 1.119\n",
      "Epoch: 26 Progress: 99% Loss: 1.191\n",
      "\n",
      "Epoch: 26 Loss: tensor(1.1923)\n",
      "\n",
      "Epoch: 27 Progress:  0% Loss: 0.036\n",
      "Epoch: 27 Progress:  4% Loss: 0.589\n",
      "Epoch: 27 Progress:  8% Loss: 1.722\n",
      "Epoch: 27 Progress: 12% Loss: 1.299\n",
      "Epoch: 27 Progress: 16% Loss: 1.078\n",
      "Epoch: 27 Progress: 20% Loss: 0.945\n",
      "Epoch: 27 Progress: 24% Loss: 1.024\n",
      "Epoch: 27 Progress: 28% Loss: 1.106\n",
      "Epoch: 27 Progress: 32% Loss: 1.245\n",
      "Epoch: 27 Progress: 36% Loss: 1.188\n",
      "Epoch: 27 Progress: 40% Loss: 1.228\n",
      "Epoch: 27 Progress: 43% Loss: 1.168\n",
      "Epoch: 27 Progress: 47% Loss: 1.119\n",
      "Epoch: 27 Progress: 51% Loss: 1.065\n",
      "Epoch: 27 Progress: 55% Loss: 1.051\n",
      "Epoch: 27 Progress: 59% Loss: 1.035\n",
      "Epoch: 27 Progress: 63% Loss: 0.999\n",
      "Epoch: 27 Progress: 67% Loss: 1.023\n",
      "Epoch: 27 Progress: 71% Loss: 1.050\n",
      "Epoch: 27 Progress: 75% Loss: 1.044\n",
      "Epoch: 27 Progress: 79% Loss: 1.136\n",
      "Epoch: 27 Progress: 83% Loss: 1.145\n",
      "Epoch: 27 Progress: 87% Loss: 1.128\n",
      "Epoch: 27 Progress: 91% Loss: 1.113\n",
      "Epoch: 27 Progress: 95% Loss: 1.114\n",
      "Epoch: 27 Progress: 99% Loss: 1.185\n",
      "\n",
      "Epoch: 27 Loss: tensor(1.1863)\n",
      "\n",
      "Epoch: 28 Progress:  0% Loss: 0.133\n",
      "Epoch: 28 Progress:  4% Loss: 0.565\n",
      "Epoch: 28 Progress:  8% Loss: 1.717\n",
      "Epoch: 28 Progress: 12% Loss: 1.295\n",
      "Epoch: 28 Progress: 16% Loss: 1.074\n",
      "Epoch: 28 Progress: 20% Loss: 0.939\n",
      "Epoch: 28 Progress: 24% Loss: 1.022\n",
      "Epoch: 28 Progress: 28% Loss: 1.098\n",
      "Epoch: 28 Progress: 32% Loss: 1.235\n",
      "Epoch: 28 Progress: 36% Loss: 1.181\n",
      "Epoch: 28 Progress: 40% Loss: 1.217\n",
      "Epoch: 28 Progress: 43% Loss: 1.160\n",
      "Epoch: 28 Progress: 47% Loss: 1.110\n",
      "Epoch: 28 Progress: 51% Loss: 1.058\n",
      "Epoch: 28 Progress: 55% Loss: 1.040\n",
      "Epoch: 28 Progress: 59% Loss: 1.026\n",
      "Epoch: 28 Progress: 63% Loss: 0.991\n",
      "Epoch: 28 Progress: 67% Loss: 1.020\n",
      "Epoch: 28 Progress: 71% Loss: 1.046\n",
      "Epoch: 28 Progress: 75% Loss: 1.039\n",
      "Epoch: 28 Progress: 79% Loss: 1.130\n",
      "Epoch: 28 Progress: 83% Loss: 1.140\n",
      "Epoch: 28 Progress: 87% Loss: 1.123\n",
      "Epoch: 28 Progress: 91% Loss: 1.108\n",
      "Epoch: 28 Progress: 95% Loss: 1.109\n",
      "Epoch: 28 Progress: 99% Loss: 1.182\n",
      "\n",
      "Epoch: 28 Loss: tensor(1.1840)\n",
      "\n",
      "Epoch: 29 Progress:  0% Loss: 0.046\n",
      "Epoch: 29 Progress:  4% Loss: 0.568\n",
      "Epoch: 29 Progress:  8% Loss: 1.704\n",
      "Epoch: 29 Progress: 12% Loss: 1.291\n",
      "Epoch: 29 Progress: 16% Loss: 1.072\n",
      "Epoch: 29 Progress: 20% Loss: 0.937\n",
      "Epoch: 29 Progress: 24% Loss: 1.021\n",
      "Epoch: 29 Progress: 28% Loss: 1.099\n",
      "Epoch: 29 Progress: 32% Loss: 1.240\n",
      "Epoch: 29 Progress: 36% Loss: 1.183\n",
      "Epoch: 29 Progress: 40% Loss: 1.223\n",
      "Epoch: 29 Progress: 43% Loss: 1.165\n",
      "Epoch: 29 Progress: 47% Loss: 1.116\n",
      "Epoch: 29 Progress: 51% Loss: 1.062\n",
      "Epoch: 29 Progress: 55% Loss: 1.045\n",
      "Epoch: 29 Progress: 59% Loss: 1.029\n",
      "Epoch: 29 Progress: 63% Loss: 0.994\n",
      "Epoch: 29 Progress: 67% Loss: 1.022\n",
      "Epoch: 29 Progress: 71% Loss: 1.046\n",
      "Epoch: 29 Progress: 75% Loss: 1.039\n",
      "Epoch: 29 Progress: 79% Loss: 1.128\n",
      "Epoch: 29 Progress: 83% Loss: 1.139\n",
      "Epoch: 29 Progress: 87% Loss: 1.122\n",
      "Epoch: 29 Progress: 91% Loss: 1.107\n",
      "Epoch: 29 Progress: 95% Loss: 1.107\n",
      "Epoch: 29 Progress: 99% Loss: 1.176\n",
      "\n",
      "Epoch: 29 Loss: tensor(1.1779)\n",
      "\n",
      "Epoch: 30 Progress:  0% Loss: 0.132\n",
      "Epoch: 30 Progress:  4% Loss: 0.588\n",
      "Epoch: 30 Progress:  8% Loss: 1.683\n",
      "Epoch: 30 Progress: 12% Loss: 1.283\n",
      "Epoch: 30 Progress: 16% Loss: 1.065\n",
      "Epoch: 30 Progress: 20% Loss: 0.934\n",
      "Epoch: 30 Progress: 24% Loss: 1.012\n",
      "Epoch: 30 Progress: 28% Loss: 1.100\n",
      "Epoch: 30 Progress: 32% Loss: 1.229\n",
      "Epoch: 30 Progress: 36% Loss: 1.177\n",
      "Epoch: 30 Progress: 40% Loss: 1.220\n",
      "Epoch: 30 Progress: 43% Loss: 1.162\n",
      "Epoch: 30 Progress: 47% Loss: 1.111\n",
      "Epoch: 30 Progress: 51% Loss: 1.058\n",
      "Epoch: 30 Progress: 55% Loss: 1.041\n",
      "Epoch: 30 Progress: 59% Loss: 1.027\n",
      "Epoch: 30 Progress: 63% Loss: 0.991\n",
      "Epoch: 30 Progress: 67% Loss: 1.018\n",
      "Epoch: 30 Progress: 71% Loss: 1.045\n",
      "Epoch: 30 Progress: 75% Loss: 1.037\n",
      "Epoch: 30 Progress: 79% Loss: 1.124\n",
      "Epoch: 30 Progress: 83% Loss: 1.135\n",
      "Epoch: 30 Progress: 87% Loss: 1.119\n",
      "Epoch: 30 Progress: 91% Loss: 1.105\n",
      "Epoch: 30 Progress: 95% Loss: 1.105\n",
      "Epoch: 30 Progress: 99% Loss: 1.171\n",
      "\n",
      "Epoch: 30 Loss: tensor(1.1725)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_only_gru=TextOnlyGRU(lang.n_words,256,128,1).cuda()\n",
    "params = text_only_gru.parameters()\n",
    "# for name, param in lstm_numeric.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(param.data) \n",
    "        \n",
    "loss_fn=torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(params, lr=0.01) \n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.97)\n",
    "# we will train for only a single epoch \n",
    "epoch = 30\n",
    "\n",
    "\n",
    "history_train_loss=[]\n",
    "history_val_loss=[]\n",
    "history_train_acc=[]\n",
    "\n",
    "# main loop\n",
    "for e in range(1,epoch+1):\n",
    "    \n",
    "    train_losses=[]\n",
    "    train_acc=[]\n",
    "    \n",
    "    running_loss=0\n",
    "    train_correct=0\n",
    "    size=0\n",
    "    \n",
    "    text_only_gru.train()\n",
    "    loss_buffer=[]\n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        size+=len(y)\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = text_only_gru(x)\n",
    "        output=output.float()\n",
    "        y=y.float()\n",
    "        loss = loss_fn(output, y)\n",
    "#         acc= correct(output,y)\n",
    "#         train_correct+=acc\n",
    "        running_loss+=loss.item()\n",
    "        loss_buffer.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        \n",
    "        if(i%500==1):\n",
    "            print(f\"Epoch: {e} Progress: {100 * i/len(train_loader):2.0f}% Loss: {torch.mean(torch.tensor(loss_buffer)):.3f}\")\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    train_loss=torch.mean(torch.tensor(loss_buffer))\n",
    "    history_train_loss.append(train_loss)\n",
    "#     train_acc=train_correct/size\n",
    "    print()\n",
    "    print(\"Epoch: \"+str(e)+\" Loss: \"+str(train_loss))\n",
    "    print()\n",
    "    \n",
    "#     text_only_gru.eval()\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         running_val_loss=0\n",
    "#         val_ac=0\n",
    "#         size=0\n",
    "#         cor=0\n",
    "#         for i, (x,y) in enumerate(val_loader):\n",
    "#             x=x.float()\n",
    "#             y=y.float()\n",
    "            \n",
    "#             x=x.cuda()\n",
    "#             y=y.cuda()\n",
    "            \n",
    "#             output=text_only_gru(x)\n",
    "#             running_val_loss+=loss_fn(output,y).item()\n",
    "#             cor+=correct(output,y)\n",
    "#             size+=len(y)\n",
    "#         val_acc=cor/size\n",
    "#         val_loss=running_val_loss/len(val_loader)\n",
    "#         if (e % 10 == 0):\n",
    "#             print(\"Val loss: \"+str(val_loss) +\" Val Acc: \"+str(val_acc))\n",
    "        \n",
    "#         history_val_loss.append(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aac58f8-7864-4230-94cf-b4bb55f20951",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
