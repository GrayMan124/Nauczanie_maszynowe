{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71bce2b9-f5c3-4421-b2c0-d774fcc0466e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import Normalize\n",
    "# from collections import namedtuple\n",
    "\n",
    "#importing Technical Analysis tools\n",
    "from ta import trend\n",
    "from ta import volatility\n",
    "from ta import momentum\n",
    "\n",
    "from utils import return_fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6eec1110-19be-4915-b713-7232ebf93a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e210bf6c-8305-4ddd-9a7f-bed33ca216f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sx5e_data=pd.read_csv(\"data\\SX5E.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8758be68-ee9c-41b5-bf05-8b84e707efff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12/10/21</td>\n",
       "      <td>4198.18</td>\n",
       "      <td>4221.57</td>\n",
       "      <td>4178.66</td>\n",
       "      <td>4199.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12/09/21</td>\n",
       "      <td>4242.66</td>\n",
       "      <td>4246.52</td>\n",
       "      <td>4201.85</td>\n",
       "      <td>4208.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12/08/21</td>\n",
       "      <td>4273.32</td>\n",
       "      <td>4288.20</td>\n",
       "      <td>4233.09</td>\n",
       "      <td>4233.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12/07/21</td>\n",
       "      <td>4157.72</td>\n",
       "      <td>4277.03</td>\n",
       "      <td>4157.72</td>\n",
       "      <td>4276.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12/06/21</td>\n",
       "      <td>4095.39</td>\n",
       "      <td>4146.16</td>\n",
       "      <td>4078.64</td>\n",
       "      <td>4137.11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Date     Open     High      Low    Close\n",
       "0  12/10/21  4198.18  4221.57  4178.66  4199.16\n",
       "1  12/09/21  4242.66  4246.52  4201.85  4208.30\n",
       "2  12/08/21  4273.32  4288.20  4233.09  4233.09\n",
       "3  12/07/21  4157.72  4277.03  4157.72  4276.20\n",
       "4  12/06/21  4095.39  4146.16  4078.64  4137.11"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sx5e_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0478fac2-175f-49c8-9ecb-a86b1c93e1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aczer\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ta\\trend.py:780: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  dip[idx] = 100 * (self._dip[idx] / value)\n",
      "C:\\Users\\aczer\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ta\\trend.py:785: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  din[idx] = 100 * (self._din[idx] / value)\n"
     ]
    }
   ],
   "source": [
    "sx5e_data = sx5e_data.sort_index(axis=0 ,ascending=False)\n",
    "sx5e_data.reset_index(drop=True,inplace=True)\n",
    "sx5e_data.drop(columns=['Date'],inplace=True)\n",
    "\n",
    "# sx5e_data['Return']=ret_fun\n",
    "sx5e_data.dropna(inplace=True)\n",
    "\n",
    "\n",
    "adi=trend.ADXIndicator(close=sx5e_data[' Close'],high=sx5e_data[' High'],low=sx5e_data[' Low'])\n",
    "m_acd=trend.MACD(close=sx5e_data[' Close'])\n",
    "\n",
    "r_si=momentum.RSIIndicator(close=sx5e_data[' Close'])\n",
    "stochastic_oscilator=momentum.StochasticOscillator(close=sx5e_data[' Close'],high=sx5e_data[' High'],low=sx5e_data[' Low'])\n",
    "williamsr=momentum.WilliamsRIndicator(high=sx5e_data[' High'],low=sx5e_data[' Low'],close=sx5e_data[' Close'])\n",
    "\n",
    "bollinger=volatility.BollingerBands(close=sx5e_data[' Close'])\n",
    "atr=volatility.AverageTrueRange(high=sx5e_data[' High'],low=sx5e_data[' Low'],close=sx5e_data[' Close'])\n",
    "\n",
    "exponential_moving_avarge9=trend.EMAIndicator(close=sx5e_data[' Close'],window=9)\n",
    "exponential_moving_avarge18=trend.EMAIndicator(close=sx5e_data[' Close'],window=18)\n",
    "exponential_moving_avarge30=trend.EMAIndicator(close=sx5e_data[' Close'],window=30)\n",
    "\n",
    "sx5e_data['ADX']=adi.adx()\n",
    "sx5e_data['MACD']=m_acd.macd()\n",
    "sx5e_data['RSI']=r_si.rsi()\n",
    "sx5e_data['Stoch Osc']=stochastic_oscilator.stoch()\n",
    "sx5e_data['Williams R']=williamsr.williams_r()\n",
    "sx5e_data['Bollinger High band']=bollinger.bollinger_hband()\n",
    "sx5e_data['Bolinger Low Band']=bollinger.bollinger_lband()\n",
    "sx5e_data['ATR']=atr.average_true_range()\n",
    "sx5e_data['EMA9']=exponential_moving_avarge9.ema_indicator()\n",
    "sx5e_data['EMA18']=exponential_moving_avarge18.ema_indicator()\n",
    "sx5e_data['EMA30']=exponential_moving_avarge30.ema_indicator()\n",
    "sx5e_data.dropna(inplace=True)\n",
    "ret_fun=return_fun(sx5e_data,1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a8c5f6d-7f73-4414-9e09-0c547cf629ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3077\n",
      "3076\n",
      "tensor([-0.2160,  0.3504,  1.3764,  ..., -1.0081, -0.5856, -0.2172])\n"
     ]
    }
   ],
   "source": [
    "type(ret_fun[1])\n",
    "# tensor_return=torch.Tensor(ret_fun)\n",
    "print(len(ret_fun))\n",
    "ret_fun.remove(None)\n",
    "print(len(ret_fun))\n",
    "tensor_ret=torch.tensor(ret_fun,dtype=torch.float32)\n",
    "print(tensor_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a8a11e0-7758-4614-8b3b-08d08a1191f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x=ret_fun\n",
    "plt.hist(x, bins=400)\n",
    "plt.ylim(0,70)\n",
    "plt.xlim(-5,5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "044086d8-45dd-4f60-bf07-127327b6920d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sx5e_data.drop(sx5e_data.tail(1).index,inplace=True)\n",
    "sx5e_tensor=torch.tensor(sx5e_data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "689b5d44-997d-4133-8fd7-c35a4f3640f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sx5e_data=torch.tensor(sx5e_data.values,dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca165190-7462-4409-89f0-ea0a3e7945f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=torch.mean(sx5e_tensor,dim=0)\n",
    "std=torch.std(sx5e_tensor,dim=0)\n",
    "sx5e_norm=(sx5e_tensor-mean)/std\n",
    "# print(sx5e_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7fdddf3-4324-4cc3-a18e-45a1a892c818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2078, 15])\n",
      "torch.Size([499, 15])\n",
      "torch.Size([499, 15])\n"
     ]
    }
   ],
   "source": [
    "train=sx5e_norm[998:]\n",
    "\n",
    "val=sx5e_norm[499:998]\n",
    "test=sx5e_norm[:499]\n",
    "print(train.shape)\n",
    "print(val.shape)\n",
    "print(test.shape)\n",
    "\n",
    "# print(train[2077])\n",
    "# print(val[498])\n",
    "# print(test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb3e6233-ac28-4cbb-86ed-a6b0428e2e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1859,  0.2549,  1.0534,  ..., -0.8024, -0.4736, -0.1868])\n"
     ]
    }
   ],
   "source": [
    "mean_ret=torch.mean(tensor_ret)\n",
    "std_ret=torch.std(tensor_ret)\n",
    "ret_norm=(tensor_ret-mean_ret)/std_ret\n",
    "print(ret_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "201d568f-15c3-4edd-b90e-8d952da29711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method type of Tensor object at 0x00000257935ACD60>\n",
      "torch.Size([499])\n",
      "torch.Size([499])\n"
     ]
    }
   ],
   "source": [
    "train_target=ret_norm[998:]\n",
    "val_target=ret_norm[499:998]\n",
    "test_target=ret_norm[:499]\n",
    "\n",
    "print(train_target.type)\n",
    "print(val_target.shape)\n",
    "print(test_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "598f2f10-7305-4c22-8d2e-53327ff5c48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=namedtuple('Data',['x','target'])\n",
    "prediction_days=5\n",
    "days_to_predict=1\n",
    "dataset=[]\n",
    "for i in range(len(sx5e_norm)-2):\n",
    "    end = i + prediction_days\n",
    "    out_end = end + days_to_predict\n",
    "\n",
    "    if(out_end>len(sx5e_norm)):\n",
    "        break\n",
    "    \n",
    "    \n",
    "    X=(sx5e_norm[i:end])\n",
    "    y=(tensor_ret[end])\n",
    "    d=data(X,y)\n",
    "    dataset.append(d)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df8b7cbf-ae1d-4e1a-b646-d677c4930fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2073\n",
      "499\n",
      "499\n"
     ]
    }
   ],
   "source": [
    "# print(len(dataset))\n",
    "\n",
    "train_set=dataset[998:]\n",
    "val_set=dataset[499:998]\n",
    "test_set=dataset[:499]\n",
    "\n",
    "print(len(train_set))\n",
    "print(len(val_set))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9c4247a-8a9d-41f9-8f7c-5141451f3a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=torch.utils.data.DataLoader(train_set, batch_size=128)\n",
    "val_loader=torch.utils.data.DataLoader(val_set, batch_size=64)\n",
    "test_loader=torch.utils.data.DataLoader(test_set, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df4159fd-c12f-4511-a801-8065a69093d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a260336f-da88-4f40-8f88-75b7b02b958c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import LSTMNumeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "029fdf0e-b675-429c-8320-8091c8cb0201",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  1  Train Loss:  2.879 Train Acc: 0.48963\n",
      "Val loss: 1.357 Val Acc 0.47295\n",
      "\n",
      "\n",
      "Epoch:  2  Train Loss:  1.785 Train Acc: 0.49686\n",
      "Val loss: 1.509 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  3  Train Loss:  1.582 Train Acc: 0.51712\n",
      "Val loss: 1.327 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  4  Train Loss:  1.512 Train Acc: 0.52822\n",
      "Val loss: 1.327 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  5  Train Loss:  1.508 Train Acc: 0.51230\n",
      "Val loss: 1.341 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  6  Train Loss:  1.515 Train Acc: 0.50989\n",
      "Val loss: 1.354 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  7  Train Loss:  1.523 Train Acc: 0.50796\n",
      "Val loss: 1.349 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  8  Train Loss:  1.524 Train Acc: 0.50844\n",
      "Val loss: 1.337 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  9  Train Loss:  1.522 Train Acc: 0.50796\n",
      "Val loss: 1.331 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  10  Train Loss:  1.520 Train Acc: 0.50844\n",
      "Val loss: 1.331 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  11  Train Loss:  1.518 Train Acc: 0.51037\n",
      "Val loss: 1.330 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  12  Train Loss:  1.516 Train Acc: 0.50796\n",
      "Val loss: 1.331 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  13  Train Loss:  1.517 Train Acc: 0.50651\n",
      "Val loss: 1.330 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  14  Train Loss:  1.515 Train Acc: 0.50603\n",
      "Val loss: 1.330 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  15  Train Loss:  1.516 Train Acc: 0.51230\n",
      "Val loss: 1.330 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  16  Train Loss:  1.514 Train Acc: 0.51037\n",
      "Val loss: 1.331 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  17  Train Loss:  1.513 Train Acc: 0.50748\n",
      "Val loss: 1.333 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  18  Train Loss:  1.515 Train Acc: 0.50169\n",
      "Val loss: 1.335 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  19  Train Loss:  1.516 Train Acc: 0.50651\n",
      "Val loss: 1.334 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  20  Train Loss:  1.518 Train Acc: 0.50555\n",
      "Val loss: 1.335 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  21  Train Loss:  1.518 Train Acc: 0.50796\n",
      "Val loss: 1.339 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  22  Train Loss:  1.521 Train Acc: 0.50458\n",
      "Val loss: 1.339 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  23  Train Loss:  1.521 Train Acc: 0.50748\n",
      "Val loss: 1.336 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  24  Train Loss:  1.519 Train Acc: 0.50603\n",
      "Val loss: 1.343 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  25  Train Loss:  1.521 Train Acc: 0.51423\n",
      "Val loss: 1.352 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  26  Train Loss:  1.524 Train Acc: 0.50217\n",
      "Val loss: 1.357 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  27  Train Loss:  1.524 Train Acc: 0.50844\n",
      "Val loss: 1.357 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  28  Train Loss:  1.523 Train Acc: 0.50699\n",
      "Val loss: 1.360 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  29  Train Loss:  1.522 Train Acc: 0.50844\n",
      "Val loss: 1.365 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  30  Train Loss:  1.521 Train Acc: 0.52195\n",
      "Val loss: 1.355 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  31  Train Loss:  1.518 Train Acc: 0.51809\n",
      "Val loss: 1.347 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  32  Train Loss:  1.513 Train Acc: 0.52436\n",
      "Val loss: 1.344 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  33  Train Loss:  1.512 Train Acc: 0.52050\n",
      "Val loss: 1.338 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  34  Train Loss:  1.508 Train Acc: 0.52243\n",
      "Val loss: 1.335 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  35  Train Loss:  1.507 Train Acc: 0.52822\n",
      "Val loss: 1.333 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  36  Train Loss:  1.506 Train Acc: 0.51520\n",
      "Val loss: 1.332 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  37  Train Loss:  1.505 Train Acc: 0.52967\n",
      "Val loss: 1.332 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  38  Train Loss:  1.505 Train Acc: 0.53256\n",
      "Val loss: 1.330 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  39  Train Loss:  1.504 Train Acc: 0.52388\n",
      "Val loss: 1.328 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  40  Train Loss:  1.502 Train Acc: 0.53063\n",
      "Val loss: 1.330 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  41  Train Loss:  1.502 Train Acc: 0.52967\n",
      "Val loss: 1.327 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  42  Train Loss:  1.501 Train Acc: 0.52243\n",
      "Val loss: 1.328 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  43  Train Loss:  1.501 Train Acc: 0.52147\n",
      "Val loss: 1.328 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  44  Train Loss:  1.501 Train Acc: 0.52822\n",
      "Val loss: 1.327 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  45  Train Loss:  1.500 Train Acc: 0.52774\n",
      "Val loss: 1.328 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  46  Train Loss:  1.500 Train Acc: 0.53015\n",
      "Val loss: 1.327 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  47  Train Loss:  1.499 Train Acc: 0.52726\n",
      "Val loss: 1.327 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  48  Train Loss:  1.499 Train Acc: 0.53690\n",
      "Val loss: 1.327 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  49  Train Loss:  1.499 Train Acc: 0.53256\n",
      "Val loss: 1.327 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  50  Train Loss:  1.498 Train Acc: 0.52677\n",
      "Val loss: 1.327 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  51  Train Loss:  1.498 Train Acc: 0.53449\n",
      "Val loss: 1.327 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  52  Train Loss:  1.498 Train Acc: 0.53449\n",
      "Val loss: 1.327 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  53  Train Loss:  1.498 Train Acc: 0.52918\n",
      "Val loss: 1.327 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  54  Train Loss:  1.498 Train Acc: 0.53739\n",
      "Val loss: 1.327 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  55  Train Loss:  1.497 Train Acc: 0.52677\n",
      "Val loss: 1.327 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  56  Train Loss:  1.497 Train Acc: 0.52918\n",
      "Val loss: 1.327 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  57  Train Loss:  1.497 Train Acc: 0.52774\n",
      "Val loss: 1.327 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  58  Train Loss:  1.497 Train Acc: 0.53401\n",
      "Val loss: 1.327 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  59  Train Loss:  1.497 Train Acc: 0.52098\n",
      "Val loss: 1.327 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  60  Train Loss:  1.497 Train Acc: 0.54028\n",
      "Val loss: 1.327 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  61  Train Loss:  1.497 Train Acc: 0.52629\n",
      "Val loss: 1.327 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  62  Train Loss:  1.497 Train Acc: 0.53401\n",
      "Val loss: 1.327 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  63  Train Loss:  1.496 Train Acc: 0.53208\n",
      "Val loss: 1.326 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  64  Train Loss:  1.496 Train Acc: 0.53401\n",
      "Val loss: 1.327 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  65  Train Loss:  1.496 Train Acc: 0.53208\n",
      "Val loss: 1.327 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  66  Train Loss:  1.496 Train Acc: 0.54124\n",
      "Val loss: 1.327 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  67  Train Loss:  1.496 Train Acc: 0.54173\n",
      "Val loss: 1.327 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  68  Train Loss:  1.496 Train Acc: 0.53787\n",
      "Val loss: 1.327 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  69  Train Loss:  1.496 Train Acc: 0.53932\n",
      "Val loss: 1.327 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  70  Train Loss:  1.496 Train Acc: 0.53497\n",
      "Val loss: 1.327 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  71  Train Loss:  1.496 Train Acc: 0.53883\n",
      "Val loss: 1.327 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  72  Train Loss:  1.496 Train Acc: 0.53690\n",
      "Val loss: 1.327 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  73  Train Loss:  1.496 Train Acc: 0.54269\n",
      "Val loss: 1.327 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  74  Train Loss:  1.496 Train Acc: 0.54028\n",
      "Val loss: 1.327 Val Acc 0.52705\n",
      "\n",
      "\n",
      "Epoch:  75  Train Loss:  1.496 Train Acc: 0.53787\n",
      "Val loss: 1.327 Val Acc 0.52705\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lstm_numeric=LSTMNumeric(15,30).cuda()\n",
    "params = lstm_numeric.parameters()\n",
    "# for name, param in lstm_numeric.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(param.data) \n",
    "        \n",
    "loss_fn=torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(params, lr=0.06) \n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.97)\n",
    "# we will train for only a single epoch \n",
    "epoch = 75\n",
    "\n",
    "\n",
    "history_train_loss=[]\n",
    "history_val_loss=[]\n",
    "history_train_acc=[]\n",
    "\n",
    "# main loop\n",
    "for e in range(1,epoch+1):\n",
    "    \n",
    "    train_losses=[]\n",
    "    train_acc=[]\n",
    "    loss_buffer=[]\n",
    "    running_loss=0\n",
    "    corr=0\n",
    "    size=0\n",
    "    \n",
    "    lstm_numeric.train()\n",
    "    \n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        size+=len(y)\n",
    "        x=x.float()\n",
    "        y=y.float()\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = lstm_numeric(x)\n",
    "\n",
    "        loss = loss_fn(output, y)\n",
    "        corr+=correct(output,y,std_ret,mean_ret)\n",
    "        loss_buffer.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        \n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    train_loss=torch.mean(torch.tensor(loss_buffer))\n",
    "    history_train_loss.append(train_loss)\n",
    "    train_acc=corr/size\n",
    "    print()\n",
    "    print(f\"Epoch:  {e}  Train Loss:  {train_loss:.3f} Train Acc: {train_acc:.5f}\")\n",
    "    \n",
    "    lstm_numeric.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_loss_buffer=[]\n",
    "        val_ac=0\n",
    "        size=0\n",
    "        cor=0\n",
    "        for i, (x,y) in enumerate(val_loader):\n",
    "            x=x.float()\n",
    "            y=y.float()\n",
    "            \n",
    "            x=x.cuda()\n",
    "            y=y.cuda()\n",
    "            \n",
    "            output=lstm_numeric(x)\n",
    "            val_loss_buffer.append(loss_fn(output,y).item())\n",
    "            cor+=correct(output,y,std_ret,mean_ret)\n",
    "            size+=len(y)\n",
    "        val_acc=cor/size\n",
    "        val_loss=torch.mean(torch.tensor(val_loss_buffer))\n",
    "        print(f\"Val loss: {val_loss:.3f} Val Acc {val_acc:.5f}\")\n",
    "        print()\n",
    "        \n",
    "        history_val_loss.append(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b62863b-8f56-468e-b860-65428528ee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lstm_numeric,'models/lstm_numeric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcb196bc-3a7f-49fa-b4bd-c0b42f623c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import TimeSeriesTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33ef73ff-905f-4acb-a65c-f619e44a0734",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model parameters\n",
    "dim_val = 512 # This can be any value divisible by n_heads. 512 is used in the original transformer paper.\n",
    "n_heads = 8 # The number of attention heads (aka parallel attention layers). dim_val must be divisible by this number\n",
    "n_decoder_layers = 4 # Number of times the decoder layer is stacked in the decoder\n",
    "n_encoder_layers = 4 # Number of times the encoder layer is stacked in the encoder\n",
    "input_size = 15 # The number of input variables. 1 if univariate forecasting.\n",
    "dec_seq_len = 14 # length of input given to decoder. Can have any integer value.\n",
    "enc_seq_len = 10 # length of input given to encoder. Can have any integer value.\n",
    "output_sequence_length = 10 # Length of the target sequence, i.e. how many time steps should your forecast cover\n",
    "max_seq_len = enc_seq_len # What's the longest sequence the model will encounter? Used to make the positional encoder\n",
    "\n",
    "model = TimeSeriesTransformer(\n",
    "    dim_val=dim_val,\n",
    "    input_size=input_size, \n",
    "    dec_seq_len=dec_seq_len,\n",
    "    batch_first = True,\n",
    "    # max_seq_len=max_seq_len,\n",
    "    out_seq_len=output_sequence_length, \n",
    "    n_decoder_layers=n_decoder_layers, \n",
    "    n_encoder_layers=n_encoder_layers,\n",
    "    n_heads=n_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e3b0299-ae5d-4e4b-95f7-a8d6ce1a2e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3076, 15])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sx5e_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e23cff60-d10e-406e-8e0e-a7bab2eea4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=sx5e_norm[0:24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17cf7bef-925c-4405-a328-5666438c3c09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 15])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e6b5e2f-95e1-40ce-8aa9-22148813e24c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 24, 15])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2c85953-fef1-41df-85a2-e21ae082fbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "src, trg, trg_y = model.get_src_trg(test,14,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbfcd16d-326c-48f4-a3d6-c2309bbe6551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7650, -0.7331, -0.6851, -0.5532, -0.5626, -0.5715, -0.5332, -0.5259,\n",
       "        -0.5366, -0.5574], dtype=torch.float64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8d3b8bd-4669-480c-8dcd-cc6b456b42fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From model.forward(): Size of src as given to forward(): torch.Size([1, 14, 15])\n",
      "From model.forward(): tgt size = torch.Size([1, 10, 1])\n",
      "From model.forward(): Size of src after input layer: torch.Size([1, 14, 512])\n",
      "From model.forward(): Size of src after pos_enc layer: torch.Size([1, 14, 512])\n",
      "From model.forward(): Size of src after encoder: torch.Size([1, 14, 512])\n",
      "From model.forward(): Size of decoder_output after linear decoder layer: torch.Size([1, 10, 512])\n",
      "From model.forward(): decoder_output shape after decoder: torch.Size([1, 10, 512])\n",
      "From model.forward(): decoder_output size after linear_mapping = torch.Size([1, 10, 1])\n"
     ]
    }
   ],
   "source": [
    "output = model(src.unsqueeze(0),trg.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09025ce5-890f-4dd7-bf6a-d43b85921c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba4b9e1-5b95-4ba8-a1f9-1cd9e75f6284",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632cfe3b-c155-4476-9b62-9e85be0e8fb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
